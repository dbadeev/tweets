{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1e51a5",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b2a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi==21.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (21.3.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (21.2.0)\r\n",
      "Requirement already satisfied: attrs==21.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (21.2.0)\r\n",
      "Requirement already satisfied: backcall==0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.2.0)\r\n",
      "Requirement already satisfied: bleach==4.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (4.1.0)\r\n",
      "Requirement already satisfied: cffi==1.15.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.15.0)\r\n",
      "Requirement already satisfied: click==8.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (8.0.3)\r\n",
      "Requirement already satisfied: contractions==0.0.58 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.0.58)\r\n",
      "Requirement already satisfied: debugpy==1.5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.5.1)\r\n",
      "Requirement already satisfied: decorator==5.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (5.1.0)\r\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (0.7.1)\r\n",
      "Requirement already satisfied: entrypoints==0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.3)\r\n",
      "Requirement already satisfied: inexactsearch==1.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (1.0.2)\r\n",
      "Requirement already satisfied: ipykernel==6.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (6.6.0)\r\n",
      "Requirement already satisfied: ipython==7.30.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (7.30.1)\r\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.2.0)\r\n",
      "Requirement already satisfied: ipywidgets==7.6.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (7.6.5)\r\n",
      "Requirement already satisfied: jedi==0.18.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.18.1)\r\n",
      "Requirement already satisfied: Jinja2==3.0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (3.0.3)\r\n",
      "Requirement already satisfied: joblib==1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (1.1.0)\r\n",
      "Requirement already satisfied: jsonschema==4.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (4.3.2)\r\n",
      "Requirement already satisfied: jupyter==1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (1.0.0)\r\n",
      "Requirement already satisfied: jupyter-client==7.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (7.1.0)\r\n",
      "Requirement already satisfied: jupyter-console==6.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (6.4.0)\r\n",
      "Requirement already satisfied: jupyter-core==4.9.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (4.9.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (0.1.2)\r\n",
      "Requirement already satisfied: jupyterlab-widgets==1.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (1.0.2)\r\n",
      "Requirement already satisfied: MarkupSafe==2.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (2.0.1)\r\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (0.1.3)\r\n",
      "Requirement already satisfied: mistune==0.8.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (0.8.4)\r\n",
      "Requirement already satisfied: nbclient==0.5.9 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (0.5.9)\r\n",
      "Requirement already satisfied: nbconvert==6.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (6.3.0)\r\n",
      "Requirement already satisfied: nbformat==5.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (5.1.3)\r\n",
      "Requirement already satisfied: nest-asyncio==1.5.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (1.5.4)\r\n",
      "Requirement already satisfied: nltk==3.6.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (3.6.7)\r\n",
      "Requirement already satisfied: notebook==6.4.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (6.4.6)\r\n",
      "Requirement already satisfied: numpy==1.21.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (1.21.5)\r\n",
      "Requirement already satisfied: packaging==21.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (21.3)\r\n",
      "Requirement already satisfied: pandas==1.3.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (1.3.5)\r\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (1.5.0)\r\n",
      "Requirement already satisfied: parso==0.8.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (0.8.3)\r\n",
      "Requirement already satisfied: pexpect==4.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (4.8.0)\r\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (0.7.5)\r\n",
      "Requirement already satisfied: prometheus-client==0.12.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (0.12.0)\r\n",
      "Requirement already satisfied: prompt-toolkit==3.0.24 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (3.0.24)\r\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (0.7.0)\r\n",
      "Requirement already satisfied: pycparser==2.21 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (2.21)\r\n",
      "Requirement already satisfied: Pygments==2.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (2.10.0)\r\n",
      "Requirement already satisfied: pyparsing==3.0.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (3.0.6)\r\n",
      "Requirement already satisfied: pyrsistent==0.18.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (0.18.0)\r\n",
      "Requirement already satisfied: pyspellchecker==0.6.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (0.6.2)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz==2021.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (2021.3)\r\n",
      "Requirement already satisfied: pyzmq==22.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (22.3.0)\r\n",
      "Requirement already satisfied: qtconsole==5.2.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (5.2.2)\r\n",
      "Requirement already satisfied: QtPy==1.11.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (1.11.3)\r\n",
      "Requirement already satisfied: regex==2021.11.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (2021.11.10)\r\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (1.0.2)\r\n",
      "Requirement already satisfied: scipy==1.7.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 59)) (1.7.3)\r\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (1.8.0)\r\n",
      "Requirement already satisfied: silpa-common==0.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 61)) (0.3)\r\n",
      "Requirement already satisfied: six==1.16.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 62)) (1.16.0)\r\n",
      "Requirement already satisfied: sklearn==0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (0.0)\r\n",
      "Requirement already satisfied: soundex==1.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 64)) (1.1.3)\r\n",
      "Requirement already satisfied: terminado==0.12.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 65)) (0.12.1)\r\n",
      "Requirement already satisfied: testpath==0.5.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 66)) (0.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl==3.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 67)) (3.0.0)\r\n",
      "Requirement already satisfied: tornado==6.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 68)) (6.1)\r\n",
      "Requirement already satisfied: tqdm==4.62.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 69)) (4.62.3)\r\n",
      "Requirement already satisfied: traitlets==5.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (5.1.1)\r\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (0.2.5)\r\n",
      "Requirement already satisfied: webencodings==0.5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 72)) (0.5.1)\r\n",
      "Requirement already satisfied: widgetsnbextension==3.5.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 73)) (3.5.2)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from contractions==0.0.58->-r requirements.txt (line 8)) (0.0.21)\r\n",
      "Requirement already satisfied: appnope in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipykernel==6.6.0->-r requirements.txt (line 14)) (0.1.2)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from ipython==7.30.1->-r requirements.txt (line 15)) (56.0.0)\r\n",
      "Requirement already satisfied: pyahocorasick in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions==0.0.58->-r requirements.txt (line 8)) (1.4.2)\r\n",
      "Requirement already satisfied: anyascii in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions==0.0.58->-r requirements.txt (line 8)) (0.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842a5d2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0324c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "import text_cleninig\n",
    "import text_processing\n",
    "import machine_learning\n",
    "import word2vec\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "'''\n",
    "In case of problems with SSL in nltk.download\n",
    "https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\n",
    "'''\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3d1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dbadeev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dbadeev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/dbadeev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/dbadeev/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83ac39",
   "metadata": {},
   "source": [
    "## Obtaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707ecb9",
   "metadata": {},
   "source": [
    "We have 3 datasets with data for positive, negative and neutral tweets stored in 3 csv files.\n",
    "Let's create a dataframe of those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4589dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = pd.DataFrame(pd.read_csv('data/processedNegative.csv').T).reset_index()\n",
    "negative = negative.rename(columns={'index': 'tweets'})\n",
    "negative['type'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7352865",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = pd.DataFrame(pd.read_csv('data/processedNeutral.csv').T).reset_index()\n",
    "neutral = neutral.rename(columns={'index': 'tweets'})\n",
    "neutral['type'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9387bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.DataFrame(pd.read_csv('data/processedPositive.csv').T).reset_index()\n",
    "positive = positive.rename(columns={'index': 'tweets'})\n",
    "positive['type'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37feb233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0             An inspiration in all aspects: Fashion     1\n1                                            fitness     1\n2    beauty and personality. :)KISSES TheFashionIcon     1\n3  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...     1\n4  Beautiful album from  the greatest unsung guit...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>An inspiration in all aspects: Fashion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fitness</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty and personality. :)KISSES TheFashionIcon</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Beautiful album from  the greatest unsung guit...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [positive, negative, neutral]\n",
    "df = pd.concat(frames)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce48e",
   "metadata": {},
   "source": [
    "### Let's create a dataframe for our results (different preprocessing techinques and different vectorizing methods). Dataframe is filled with NaN at this moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442253ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts TFIDF\njust tokenization                                   NaN         NaN   NaN\nstemming                                            NaN         NaN   NaN\nlemmatization                                       NaN         NaN   NaN\nstemming + misspellings                             NaN         NaN   NaN\nlemmatization + misspellings                        NaN         NaN   NaN\nany other ideas                                     NaN         NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing = ['just tokenization', 'stemming', 'lemmatization', 'stemming + misspellings',\n",
    "                                                                    'lemmatization + misspellings', 'any other ideas']\n",
    "vectorizers = ['0 or 1, if the word exists', 'word counts', 'TFIDF']\n",
    "df_df = pd.DataFrame(columns=vectorizers, index=preprocessing)\n",
    "df_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97de009",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff2b02",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b689a1",
   "metadata": {},
   "source": [
    "##### Text cleaning funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95f222",
   "metadata": {},
   "source": [
    "- contractions to full form\n",
    "- replace_emoticons with text\n",
    "- remove ticks and next symbol\n",
    "- remove url (http*)\n",
    "- remove hashtags (#)\n",
    "- remove mentions (@)\n",
    "- remove numbers\n",
    "- ignore case\n",
    "- ignore punctuation\n",
    "- remove stop words (optional)\n",
    "- remove misspelling (optional)\n",
    "- remove extra spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b3ecc",
   "metadata": {},
   "source": [
    "### Just Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85e33",
   "metadata": {},
   "source": [
    "Tokenization is a common task a data scientist comes across when working with text data. It consists of splitting an entire text into small units, also known as tokens. Most Natural Language Processing (NLP) projects have tokenization as the first step because it’s the foundation for developing good models and helps better understand the text we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bbee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2576",
   "metadata": {},
   "source": [
    "#### apply clean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f005d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token['tweets'] = df_token.apply(lambda item: text_cleninig.clean(item.tweets), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5044330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0              an inspiration in all aspects fashion     1\n1                                            fitness     1\n2  beauty and personality happy face or smiley ki...     1\n3  apka apna awam ka channel frankline tv aam adm...     1\n4  beautiful album from the greatest unsung guita...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspiration in all aspects fashion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fitness</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty and personality happy face or smiley ki...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca2b81",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd34f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_exist = text_processing.word_exists(df_token, 'tweets')\n",
    "df_df['0 or 1, if the word exists'][0] = df_token_exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06159a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    aa  aah  aam  aamby  \\\ntweets                                                                    \nan inspiration in all aspects fashion                0    0    0      0   \nfitness                                              0    0    0      0   \nbeauty and personality happy face or smiley kis...   0    0    0      0   \napka apna awam ka channel frankline tv aam admi...   0    0    1      0   \nbeautiful album from the greatest unsung guitar...   0    0    0      0   \n\n                                                    aando  aap  aaree  \\\ntweets                                                                  \nan inspiration in all aspects fashion                   0    0      0   \nfitness                                                 0    0      0   \nbeauty and personality happy face or smiley kis...      0    0      0   \napka apna awam ka channel frankline tv aam admi...      0    0      0   \nbeautiful album from the greatest unsung guitar...      0    0      0   \n\n                                                    abbeydale  abbreviation  \\\ntweets                                                                        \nan inspiration in all aspects fashion                       0             0   \nfitness                                                     0             0   \nbeauty and personality happy face or smiley kis...          0             0   \napka apna awam ka channel frankline tv aam admi...          0             0   \nbeautiful album from the greatest unsung guitar...          0             0   \n\n                                                    abc  ...  yr  yummy  yura  \\\ntweets                                                   ...                    \nan inspiration in all aspects fashion                 0  ...   0      0     0   \nfitness                                               0  ...   0      0     0   \nbeauty and personality happy face or smiley kis...    0  ...   0      0     0   \napka apna awam ka channel frankline tv aam admi...    0  ...   0      0     0   \nbeautiful album from the greatest unsung guitar...    0  ...   0      0     0   \n\n                                                    yuri  zabardast  zac  zcc  \\\ntweets                                                                          \nan inspiration in all aspects fashion                  0          0    0    0   \nfitness                                                0          0    0    0   \nbeauty and personality happy face or smiley kis...     0          0    0    0   \napka apna awam ka channel frankline tv aam admi...     0          0    0    0   \nbeautiful album from the greatest unsung guitar...     0          0    0    0   \n\n                                                    zero  zoo  zoos  \ntweets                                                               \nan inspiration in all aspects fashion                  0    0     0  \nfitness                                                0    0     0  \nbeauty and personality happy face or smiley kis...     0    0     0  \napka apna awam ka channel frankline tv aam admi...     0    0     0  \nbeautiful album from the greatest unsung guitar...     0    0     0  \n\n[5 rows x 6158 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aah</th>\n      <th>aam</th>\n      <th>aamby</th>\n      <th>aando</th>\n      <th>aap</th>\n      <th>aaree</th>\n      <th>abbeydale</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>yummy</th>\n      <th>yura</th>\n      <th>yuri</th>\n      <th>zabardast</th>\n      <th>zac</th>\n      <th>zcc</th>\n      <th>zero</th>\n      <th>zoo</th>\n      <th>zoos</th>\n    </tr>\n    <tr>\n      <th>tweets</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an inspiration in all aspects fashion</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fitness</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>beauty and personality happy face or smiley kisses thefashionicon</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>apka apna awam ka channel frankline tv aam admi production please visit or likes share happy face or smiley fb page</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>beautiful album from the greatest unsung guitar genius of our time and i have met the great backstage</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 6158 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_exist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc87ed0",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba87a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count = text_processing.word_count(df_token, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e27f904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df['word counts'][0] = df_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccfc81",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f55db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_tfidf = text_processing.tfidf(df_token, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c6b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df['TFIDF'][0] = df_token_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671572a",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc34d80",
   "metadata": {},
   "source": [
    "Stemming: This removes the difference between the inflected form of a word to reduce each word to its root form. This is done by mostly chopping off the end of words. One problem with streaming is that chopping words may result in words that are not part of the vocabulary. PorterStemmer and LancasterStemmer are two popular algorithms for streaming, which have rules on how to chop off a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c8c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe9c17",
   "metadata": {},
   "source": [
    "Let's consider several stemmers: Porter Stemmer, Snowball Stemmer and Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5011839",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11128b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0                    an inspir in all aspect fashion     1\n1                                                fit     1\n2  beauti and person happi face or smiley kiss th...     1\n3  apka apna awam ka channel franklin tv aam admi...     1\n4  beauti album from the greatest unsung guitar g...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in all aspect fashion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauti and person happi face or smiley kiss th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>apka apna awam ka channel franklin tv aam admi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed_snow = df.copy(deep=True)\n",
    "df_stemmed_snow['tweets'] = df_stemmed_snow.apply(lambda item: text_processing.stem_text(item.tweets, snowball), axis=1)\n",
    "df_stemmed_snow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc831afe",
   "metadata": {},
   "source": [
    "#### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa89d31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0                        an inspir in al aspect fash     1\n1                                                fit     1\n2  beauty and person happy fac or smiley kiss the...     1\n3  apk apn awam ka channel franklin tv aam adm pr...     1\n4  beauty alb from the greatest unsung guit geni ...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in al aspect fash</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty and person happy fac or smiley kiss the...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>apk apn awam ka channel franklin tv aam adm pr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beauty alb from the greatest unsung guit geni ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed_lanc = df.copy(deep=True)\n",
    "df_stemmed_lanc['tweets'] = df_stemmed_lanc.apply(lambda item: text_processing.stem_text(item.tweets, lancaster), axis=1)\n",
    "df_stemmed_lanc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475d2ef",
   "metadata": {},
   "source": [
    "#### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90c53df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0                    an inspir in all aspect fashion     1\n1                                                fit     1\n2  beauti and person happi face or smiley kiss th...     1\n3  apka apna awam ka channel franklin tv aam admi...     1\n4  beauti album from the greatest unsung guitar g...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in all aspect fashion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauti and person happi face or smiley kiss th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>apka apna awam ka channel franklin tv aam admi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed_porter = df.copy(deep=True)\n",
    "df_stemmed_porter['tweets'] = df_stemmed_porter.apply(lambda item: text_processing.stem_text(item.tweets, porter), axis=1)\n",
    "df_stemmed_porter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883beec",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f4a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_exist = text_processing.word_exists(df_stemmed_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca1386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4927 entries, aa to zoo\n",
      "dtypes: int64(4927)\n",
      "memory usage: 145.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_exist.info()\n",
    "df_df['0 or 1, if the word exists'][1] = df_stem_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507c8ca",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ca364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_count = text_processing.word_count(df_stemmed_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f61381d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4927 entries, aa to zoo\n",
      "dtypes: int64(4927)\n",
      "memory usage: 145.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_count.info()\n",
    "df_df['word counts'][1] = df_stem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13a4a2",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "717cdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_tfidf = text_processing.tfidf(df_stemmed_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac256dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4927 entries, aa to zoo\n",
      "dtypes: float64(4927)\n",
      "memory usage: 145.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_tfidf.info()\n",
    "df_df['TFIDF'][1] = df_stem_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fb149",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9664b04",
   "metadata": {},
   "source": [
    "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebcd56b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0               an inspiration in all aspect fashion     1\n1                                            fitness     1\n2  beauty and personality happy face or smiley ki...     1\n3  apka apna awam ka channel frankline tv aam adm...     1\n4  beautiful album from the greatest unsung guita...     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspiration in all aspect fashion</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fitness</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty and personality happy face or smiley ki...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = df.copy(deep=True)\n",
    "df_lemmatized['tweets'] = df_lemmatized.apply(lambda item: text_processing.lem_text(item.tweets), axis=1)\n",
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df842bea",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eff4d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_exist = text_processing.word_exists(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0468962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5632 entries, aa to zoo\n",
      "dtypes: int64(5632)\n",
      "memory usage: 166.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_exist.info()\n",
    "df_df['0 or 1, if the word exists'][2] = df_lem_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0fad7",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5a4b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_count = text_processing.word_count(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1c7ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5632 entries, aa to zoo\n",
      "dtypes: int64(5632)\n",
      "memory usage: 166.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_count.info()\n",
    "df_df['word counts'][2] = df_lem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7774d",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4ea30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_tfidf = text_processing.tfidf(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83c0dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5632 entries, aa to zoo\n",
      "dtypes: float64(5632)\n",
      "memory usage: 166.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_tfidf.info()\n",
    "df_df['TFIDF'][2] = df_lem_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a431b",
   "metadata": {},
   "source": [
    "### Stemming + misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0274883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_spell_snow = df.copy(deep=True)\n",
    "df_stem_spell_snow['tweets'] = df_stem_spell_snow.apply(lambda item: text_processing.stem_text(item.tweets, snowball, misspelling=True), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eba810",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b4883b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_spell_exist = text_processing.word_exists(df_stem_spell_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ded213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulet patsak has been appoint new delhi polic commission patsak is a gamut cadr ip offic\n",
      "Columns: 4553 entries, abb to zoo\n",
      "dtypes: int64(4553)\n",
      "memory usage: 134.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_exist.info()\n",
    "df_df['0 or 1, if the word exists'][3] = df_stem_spell_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7285c1",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a84f8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_spell_count = text_processing.word_count(df_stemmed_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b829744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4927 entries, aa to zoo\n",
      "dtypes: int64(4927)\n",
      "memory usage: 145.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_count.info()\n",
    "df_df['word counts'][3] = df_stem_spell_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80829d0d",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea3b20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem_spell_tfidf = text_processing.tfidf(df_stemmed_snow, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b697c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspir in all aspect fashion to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4927 entries, aa to zoo\n",
      "dtypes: float64(4927)\n",
      "memory usage: 145.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_tfidf.info()\n",
    "df_df['TFIDF'][3] = df_stem_spell_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e775c0c",
   "metadata": {},
   "source": [
    "### Lemmatization + misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7804edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = df.copy(deep=True)\n",
    "df_lemmatized['tweets'] = df_lemmatized.apply(lambda item: text_processing.lem_text(item.tweets, misspelling=True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8bba2",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc212736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_spell_exist = text_processing.word_exists(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "545f0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulet patsak ha been appointed new delhi police commissioner patsak is a gamut cadre ip officer\n",
      "Columns: 5275 entries, abb to zoo\n",
      "dtypes: int64(5275)\n",
      "memory usage: 155.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_exist.info()\n",
    "df_df['0 or 1, if the word exists'][4] = df_lem_spell_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230ed30",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8597d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_spell_count = text_processing.word_count(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0d4fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulet patsak ha been appointed new delhi police commissioner patsak is a gamut cadre ip officer\n",
      "Columns: 5275 entries, abb to zoo\n",
      "dtypes: int64(5275)\n",
      "memory usage: 155.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_count.info()\n",
    "df_df['word counts'][4] = df_lem_spell_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507c7e8",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc30511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_spell_tfidf = text_processing.tfidf(df_lemmatized, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "169321bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3873 entries, an inspiration in all aspect fashion to amulet patsak ha been appointed new delhi police commissioner patsak is a gamut cadre ip officer\n",
      "Columns: 5275 entries, abb to zoo\n",
      "dtypes: float64(5275)\n",
      "memory usage: 155.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_tfidf.info()\n",
    "df_df['TFIDF'][4] = df_lem_spell_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062bffc",
   "metadata": {},
   "source": [
    "### Other ideas of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6244847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e4c7aad",
   "metadata": {},
   "source": [
    "> ### 0 or 1, if the word exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6339c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d71b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc2a3e7",
   "metadata": {},
   "source": [
    "> ### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9b056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca21ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11258b99",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15751c5c",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57ad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ed302e",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934af43",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e72d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "466c42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import machine_learning\n",
    "\n",
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c401d752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts     TFIDF\njust tokenization                              0.903226    0.904516  0.889032\nstemming                                       0.909677    0.905806  0.892903\nlemmatization                                  0.900645    0.900645  0.887742\nstemming + misspellings                        0.908387    0.905806  0.892903\nlemmatization + misspellings                   0.901935    0.903226  0.887742\nany other ideas                                     NaN         NaN       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.903226</td>\n      <td>0.904516</td>\n      <td>0.889032</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.909677</td>\n      <td>0.905806</td>\n      <td>0.892903</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.900645</td>\n      <td>0.900645</td>\n      <td>0.887742</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>0.908387</td>\n      <td>0.905806</td>\n      <td>0.892903</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.901935</td>\n      <td>0.903226</td>\n      <td>0.887742</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef7a46",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48fdef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f9b3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f41da16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts     TFIDF\njust tokenization                              0.749677    0.750968  0.727742\nstemming                                       0.775484    0.772903  0.732903\nlemmatization                                  0.763871    0.766452  0.723871\nstemming + misspellings                        0.749677    0.772903  0.732903\nlemmatization + misspellings                   0.745806    0.745806  0.699355\nany other ideas                                     NaN         NaN       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.749677</td>\n      <td>0.750968</td>\n      <td>0.727742</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.775484</td>\n      <td>0.772903</td>\n      <td>0.732903</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.763871</td>\n      <td>0.766452</td>\n      <td>0.723871</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>0.749677</td>\n      <td>0.772903</td>\n      <td>0.732903</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.745806</td>\n      <td>0.745806</td>\n      <td>0.699355</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e01737",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f29893f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74ed4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a4a9b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts     TFIDF\njust tokenization                              0.460645    0.460645  0.455484\nstemming                                       0.458065    0.458065  0.460645\nlemmatization                                   0.47871     0.47871  0.474839\nstemming + misspellings                            0.48    0.458065  0.460645\nlemmatization + misspellings                   0.469677    0.469677  0.468387\nany other ideas                                     NaN         NaN       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.460645</td>\n      <td>0.460645</td>\n      <td>0.455484</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.458065</td>\n      <td>0.458065</td>\n      <td>0.460645</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.47871</td>\n      <td>0.47871</td>\n      <td>0.474839</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>0.48</td>\n      <td>0.458065</td>\n      <td>0.460645</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.469677</td>\n      <td>0.469677</td>\n      <td>0.468387</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 1000, criterion = 'entropy', random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts     TFIDF\njust tokenization                              0.910968    0.910968  0.908387\nstemming                                       0.910968    0.912258  0.907097\nlemmatization                                  0.907097    0.904516  0.904516\nstemming + misspellings                        0.903226    0.912258  0.907097\nlemmatization + misspellings                   0.908387    0.905806  0.905806\nany other ideas                                     NaN         NaN       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.910968</td>\n      <td>0.910968</td>\n      <td>0.908387</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.910968</td>\n      <td>0.912258</td>\n      <td>0.907097</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.907097</td>\n      <td>0.904516</td>\n      <td>0.904516</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>0.903226</td>\n      <td>0.912258</td>\n      <td>0.907097</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.908387</td>\n      <td>0.905806</td>\n      <td>0.905806</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "4634b58f",
   "metadata": {},
   "source": [
    "### Desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "348593ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9eb6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4ce2994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                             0 or 1, if the word exists word counts     TFIDF\njust tokenization                              0.877419    0.876129  0.882581\nstemming                                       0.876129    0.872258  0.883871\nlemmatization                                  0.865806    0.870968  0.872258\nstemming + misspellings                            0.88    0.874839  0.877419\nlemmatization + misspellings                   0.877419    0.882581  0.872258\nany other ideas                                     NaN         NaN       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.877419</td>\n      <td>0.876129</td>\n      <td>0.882581</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.876129</td>\n      <td>0.872258</td>\n      <td>0.883871</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.865806</td>\n      <td>0.870968</td>\n      <td>0.872258</td>\n    </tr>\n    <tr>\n      <th>stemming + misspellings</th>\n      <td>0.88</td>\n      <td>0.874839</td>\n      <td>0.877419</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.877419</td>\n      <td>0.882581</td>\n      <td>0.872258</td>\n    </tr>\n    <tr>\n      <th>any other ideas</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706d6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddab8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e41a7303",
   "metadata": {},
   "source": [
    "## Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd593ab",
   "metadata": {},
   "source": [
    "- final table with all algos and parameters and scores exist\n",
    "- all 18 means of preprocessing used and corresponding datasets saved\n",
    "- top 10 most similar tweets found for all 18 means of preprocessing\n",
    "- >0.83\n",
    "- grid search was used for finding best params\n",
    "- word-to-vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d277e",
   "metadata": {},
   "source": [
    "### Some useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9abf4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "- sentiment analysis https://www.analyticsvidhya.com/blog/2021/09/sentiment-classification-using-nlp-with-text-analytics/\n",
    "- https://becominghuman.ai/nlp-classifying-positive-and-negative-restaurant-reviews-bag-of-words-model-31e9abfd7286\n",
    "- Comments classification https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "- tokenization https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4\n",
    "- Lemmatization https://pythobyte.com/stemming-and-lemmatization-82464/\n",
    "- Fundamentals of Bag Of Words and TF-IDF https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22\n",
    "- How to Vectorize Text in DataFrames for NLP Tasks https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "- Stemming и лемматизация в Python https://pythobyte.com/stemming-and-lemmatization-82464/\n",
    "- https://www.bigdataschool.ru/blog/pyspark-vectorization.html\n",
    "- https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n",
    "- https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "- different stemmers https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/\n",
    "- preprocessing https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660724"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}