{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1e51a5",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b2a699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T21:59:56.601018Z",
     "start_time": "2024-08-03T21:59:50.688151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==1.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: anyascii==0.3.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.3.0)\r\n",
      "Requirement already satisfied: appnope==0.1.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.1.2)\r\n",
      "Requirement already satisfied: argon2-cffi==21.3.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (21.3.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (21.2.0)\r\n",
      "Requirement already satisfied: asttokens==2.0.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.0.5)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.6.3)\r\n",
      "Requirement already satisfied: attrs==21.4.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (21.4.0)\r\n",
      "Requirement already satisfied: autocorrect==2.6.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.6.1)\r\n",
      "Requirement already satisfied: backcall==0.2.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.2.0)\r\n",
      "Requirement already satisfied: black==21.12b0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (21.12b0)\r\n",
      "Requirement already satisfied: bleach==4.1.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (4.1.0)\r\n",
      "Requirement already satisfied: cachetools==4.2.4 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (4.2.4)\r\n",
      "Requirement already satisfied: certifi==2021.10.8 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (2021.10.8)\r\n",
      "Requirement already satisfied: cffi==1.15.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (1.15.0)\r\n",
      "Requirement already satisfied: charset-normalizer==2.0.10 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (2.0.10)\r\n",
      "Requirement already satisfied: click==8.0.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (8.0.3)\r\n",
      "Requirement already satisfied: contractions==0.0.58 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.0.58)\r\n",
      "Requirement already satisfied: debugpy==1.5.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (1.5.1)\r\n",
      "Requirement already satisfied: decorator==5.1.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (5.1.1)\r\n",
      "Requirement already satisfied: defusedxml==0.7.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (0.7.1)\r\n",
      "Requirement already satisfied: entrypoints==0.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (0.3)\r\n",
      "Requirement already satisfied: executing==0.8.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (0.8.2)\r\n",
      "Requirement already satisfied: flatbuffers==2.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (2.0)\r\n",
      "Requirement already satisfied: gast==0.4.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.4.0)\r\n",
      "Requirement already satisfied: gensim==4.1.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (4.1.2)\r\n",
      "Requirement already satisfied: google-auth==2.3.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (2.3.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (0.4.6)\r\n",
      "Requirement already satisfied: google-pasta==0.2.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio==1.43.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.43.0)\r\n",
      "Requirement already satisfied: h5py==3.6.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (3.6.0)\r\n",
      "Requirement already satisfied: idna==3.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (3.3)\r\n",
      "Requirement already satisfied: importlib-metadata==4.10.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (4.10.0)\r\n",
      "Requirement already satisfied: inexactsearch==1.0.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (1.0.2)\r\n",
      "Requirement already satisfied: ipykernel==6.6.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (6.6.1)\r\n",
      "Requirement already satisfied: ipython==7.31.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (7.31.0)\r\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (0.2.0)\r\n",
      "Requirement already satisfied: ipywidgets==7.6.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (7.6.5)\r\n",
      "Requirement already satisfied: jedi==0.18.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (0.18.1)\r\n",
      "Requirement already satisfied: Jinja2==3.0.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (3.0.3)\r\n",
      "Requirement already satisfied: joblib==1.1.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (1.1.0)\r\n",
      "Requirement already satisfied: jsonschema==4.4.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (4.4.0)\r\n",
      "Requirement already satisfied: jupyter==1.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (1.0.0)\r\n",
      "Requirement already satisfied: jupyter-client==7.1.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (7.1.0)\r\n",
      "Requirement already satisfied: jupyter-console==6.4.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (6.4.0)\r\n",
      "Requirement already satisfied: jupyter-core==4.9.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (4.9.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (0.1.2)\r\n",
      "Requirement already satisfied: jupyterlab-widgets==1.0.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (1.0.2)\r\n",
      "Requirement already satisfied: keras==2.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (2.7.0)\r\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (1.1.2)\r\n",
      "Requirement already satisfied: libclang==12.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (12.0.0)\r\n",
      "Requirement already satisfied: Markdown==3.3.6 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (3.3.6)\r\n",
      "Requirement already satisfied: MarkupSafe==2.0.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (2.0.1)\r\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (0.1.3)\r\n",
      "Requirement already satisfied: mistune==0.8.4 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (0.8.4)\r\n",
      "Requirement already satisfied: mypy-extensions==0.4.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.4.3)\r\n",
      "Requirement already satisfied: nbclient==0.5.9 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (0.5.9)\r\n",
      "Requirement already satisfied: nbconvert==6.4.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (6.4.0)\r\n",
      "Requirement already satisfied: nbformat==5.1.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 59)) (5.1.3)\r\n",
      "Requirement already satisfied: nest-asyncio==1.5.4 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (1.5.4)\r\n",
      "Requirement already satisfied: nltk==3.6.7 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 61)) (3.6.7)\r\n",
      "Requirement already satisfied: notebook==6.4.7 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 62)) (6.4.7)\r\n",
      "Requirement already satisfied: numpy==1.21.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (1.21.5)\r\n",
      "Requirement already satisfied: oauthlib==3.1.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 64)) (3.1.1)\r\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 65)) (3.3.0)\r\n",
      "Requirement already satisfied: packaging==21.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 66)) (21.3)\r\n",
      "Requirement already satisfied: pandas==1.3.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 67)) (1.3.5)\r\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 68)) (1.5.0)\r\n",
      "Requirement already satisfied: parso==0.8.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 69)) (0.8.3)\r\n",
      "Requirement already satisfied: pathspec==0.9.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (0.9.0)\r\n",
      "Requirement already satisfied: pexpect==4.8.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (4.8.0)\r\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 72)) (0.7.5)\r\n",
      "Requirement already satisfied: platformdirs==2.4.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 73)) (2.4.1)\r\n",
      "Requirement already satisfied: prometheus-client==0.12.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 74)) (0.12.0)\r\n",
      "Requirement already satisfied: prompt-toolkit==3.0.24 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 75)) (3.0.24)\r\n",
      "Requirement already satisfied: protobuf==3.19.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 76)) (3.19.3)\r\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 77)) (0.7.0)\r\n",
      "Requirement already satisfied: pure-eval==0.2.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 78)) (0.2.1)\r\n",
      "Requirement already satisfied: pyahocorasick==1.4.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (1.4.2)\r\n",
      "Requirement already satisfied: pyasn1==0.4.8 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 80)) (0.4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 81)) (0.2.8)\r\n",
      "Requirement already satisfied: pycparser==2.21 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 82)) (2.21)\r\n",
      "Requirement already satisfied: Pygments==2.11.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 83)) (2.11.2)\r\n",
      "Requirement already satisfied: pyparsing==3.0.6 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 84)) (3.0.6)\r\n",
      "Requirement already satisfied: pyrsistent==0.18.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 85)) (0.18.0)\r\n",
      "Requirement already satisfied: pyspellchecker==0.6.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 86)) (0.6.2)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 87)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz==2021.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 88)) (2021.3)\r\n",
      "Requirement already satisfied: pyzmq==22.3.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 89)) (22.3.0)\r\n",
      "Requirement already satisfied: qtconsole==5.2.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 90)) (5.2.2)\r\n",
      "Requirement already satisfied: QtPy==2.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 91)) (2.0.0)\r\n",
      "Requirement already satisfied: regex==2021.11.10 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 92)) (2021.11.10)\r\n",
      "Requirement already satisfied: requests==2.27.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 93)) (2.27.1)\r\n",
      "Requirement already satisfied: requests-oauthlib==1.3.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 94)) (1.3.0)\r\n",
      "Requirement already satisfied: rsa==4.8 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 95)) (4.8)\r\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 96)) (1.0.2)\r\n",
      "Requirement already satisfied: scipy==1.7.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 97)) (1.7.3)\r\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 98)) (1.8.0)\r\n",
      "Requirement already satisfied: silpa-common==0.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 99)) (0.3)\r\n",
      "Requirement already satisfied: six==1.16.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 100)) (1.16.0)\r\n",
      "Requirement already satisfied: sklearn==0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 101)) (0.0)\r\n",
      "Requirement already satisfied: smart-open==5.2.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 102)) (5.2.1)\r\n",
      "Requirement already satisfied: soundex==1.1.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 103)) (1.1.3)\r\n",
      "Requirement already satisfied: spellchecker==0.4 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 104)) (0.4)\r\n",
      "Requirement already satisfied: stack-data==0.1.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 105)) (0.1.3)\r\n",
      "Requirement already satisfied: tensorboard==2.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 106)) (2.7.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 107)) (0.6.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 108)) (1.8.1)\r\n",
      "Requirement already satisfied: tensorflow==2.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 109)) (2.7.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator==2.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 110)) (2.7.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.23.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 111)) (0.23.1)\r\n",
      "Requirement already satisfied: termcolor==1.1.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 112)) (1.1.0)\r\n",
      "Requirement already satisfied: terminado==0.12.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 113)) (0.12.1)\r\n",
      "Requirement already satisfied: testpath==0.5.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 114)) (0.5.0)\r\n",
      "Requirement already satisfied: textblob==0.17.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 115)) (0.17.1)\r\n",
      "Requirement already satisfied: textsearch==0.0.21 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 116)) (0.0.21)\r\n",
      "Requirement already satisfied: threadpoolctl==3.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 117)) (3.0.0)\r\n",
      "Requirement already satisfied: tomli==1.2.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 118)) (1.2.3)\r\n",
      "Requirement already satisfied: tornado==6.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 119)) (6.1)\r\n",
      "Requirement already satisfied: tqdm==4.62.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 120)) (4.62.3)\r\n",
      "Requirement already satisfied: traitlets==5.1.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 121)) (5.1.1)\r\n",
      "Requirement already satisfied: typing_extensions==4.0.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 122)) (4.0.1)\r\n",
      "Requirement already satisfied: urllib3==1.26.8 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 123)) (1.26.8)\r\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 124)) (0.2.5)\r\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 125)) (0.5.1)\r\n",
      "Requirement already satisfied: Werkzeug==2.0.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 126)) (2.0.2)\r\n",
      "Requirement already satisfied: widgetsnbextension==3.5.2 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 127)) (3.5.2)\r\n",
      "Requirement already satisfied: wrapt==1.13.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 128)) (1.13.3)\r\n",
      "Requirement already satisfied: zipp==3.7.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 129)) (3.7.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.9/site-packages (from astunparse==1.6.3->-r requirements.txt (line 7)) (0.41.2)\r\n",
      "Requirement already satisfied: setuptools>=40.3.0 in ./venv/lib/python3.9/site-packages (from google-auth==2.3.3->-r requirements.txt (line 27)) (68.2.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a7530e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T21:59:56.602595Z",
     "start_time": "2024-08-03T21:59:55.784393Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842a5d2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0324c63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:06.645603Z",
     "start_time": "2024-08-03T21:59:55.805949Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import text_cleaninig\n",
    "import text_processing\n",
    "import machine_learning\n",
    "import w2v_ml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5560ed7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:06.723208Z",
     "start_time": "2024-08-03T22:00:06.647554Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In case of problems with SSL in nltk.download\n",
    "https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\n",
    "'''\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3d1962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:07.310234Z",
     "start_time": "2024-08-03T22:00:06.714604Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dnb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/dnb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/dnb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/dnb/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83ac39",
   "metadata": {},
   "source": [
    "## Obtaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707ecb9",
   "metadata": {},
   "source": [
    "There are 3 datasets with positive, negative and neutral tweets stored in csv files.\n",
    "Let's create a dataframe of those data. Negative tweets will have a type of -1, positive ones will have a type of 1, and neutral ones will have a type of 0. Duplicate tweets are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4589dd17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:07.455330Z",
     "start_time": "2024-08-03T22:00:07.284021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0              How unhappy  some dogs like it though    -1\n1  talking to my over driver about where I'm goin...    -1\n2  Does anybody know if the Rand's likely to fall...    -1\n3         I miss going to gigs in Liverpool unhappy     -1\n4      There isnt a new Riverdale tonight ? unhappy     -1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How unhappy  some dogs like it though</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>talking to my over driver about where I'm goin...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Does anybody know if the Rand's likely to fall...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I miss going to gigs in Liverpool unhappy</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>There isnt a new Riverdale tonight ? unhappy</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/processedNegative.csv') as file:\n",
    "    data = file.read()\n",
    "split_pattern = r',(?=[^ ])'\n",
    "data = re.sub(split_pattern, '||', data)\n",
    "tweets = data.split('||')\n",
    "negative = pd.DataFrame(tweets, columns=['tweets'])\n",
    "negative['type'] = -1\n",
    "negative.drop_duplicates(inplace=True)\n",
    "negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7352865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:07.680925Z",
     "start_time": "2024-08-03T22:00:07.386126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  Pak PM survives removal scare, but court order...     0\n1  Supreme Court quashes criminal complaint again...     0\n2  Art of Living's fights back over Yamuna floodp...     0\n3  FCRA slap on NGO for lobbying...But was it doi...     0\n4  Why doctors, pharma companies are opposing nam...     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pak PM survives removal scare, but court order...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Supreme Court quashes criminal complaint again...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Art of Living's fights back over Yamuna floodp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FCRA slap on NGO for lobbying...But was it doi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Why doctors, pharma companies are opposing nam...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/processedNeutral.csv') as file:\n",
    "    data = file.read()\n",
    "split_pattern = r',(?=[^ ])'\n",
    "data = re.sub(split_pattern, '||', data)\n",
    "tweets = data.split('||')\n",
    "neutral = pd.DataFrame(tweets, columns=['tweets'])\n",
    "neutral['type'] = 0\n",
    "neutral.drop_duplicates(inplace=True)\n",
    "neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9387bcd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:07.720233Z",
     "start_time": "2024-08-03T22:00:07.488307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  An inspiration in all aspects: Fashion, fitnes...     1\n1  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...     1\n2  Beautiful album from  the greatest unsung guit...     1\n3  Good luck to Rich riding for great project in ...     1\n4            Omg he... kissed... him crying with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>An inspiration in all aspects: Fashion, fitnes...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Beautiful album from  the greatest unsung guit...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good luck to Rich riding for great project in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Omg he... kissed... him crying with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/processedPositive.csv') as file:\n",
    "    data = file.read()\n",
    "split_pattern = r',(?=[^ ])'\n",
    "data = re.sub(split_pattern, '||', data)\n",
    "tweets = data.split('||')\n",
    "positive = pd.DataFrame(tweets, columns=['tweets'])\n",
    "positive['type'] = 1\n",
    "positive.drop_duplicates(inplace=True)\n",
    "positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37feb233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:07.920242Z",
     "start_time": "2024-08-03T22:00:07.561288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  An inspiration in all aspects: Fashion, fitnes...     1\n1  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...     1\n2  Beautiful album from  the greatest unsung guit...     1\n3  Good luck to Rich riding for great project in ...     1\n4            Omg he... kissed... him crying with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>An inspiration in all aspects: Fashion, fitnes...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Beautiful album from  the greatest unsung guit...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good luck to Rich riding for great project in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Omg he... kissed... him crying with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [positive, negative, neutral]\n",
    "df = pd.concat(frames)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce48e",
   "metadata": {},
   "source": [
    "### Let's create a dataframe with the results for different preprocessing methods and different vectorization methods. Snowball is used for _stemming_ and _stemming(snow) + misspellings_, Lancaster is used for _stemming(lanc) + misspellings_ (as Task's Other ideas of preprocessing') in text preprocessing for stemming.The initial values of the dataframe cells are NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "442253ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:08.200024Z",
     "start_time": "2024-08-03T22:00:07.689768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              0 or 1, if the word exists word counts TFIDF\njust tokenization                                    NaN         NaN   NaN\nstemming                                             NaN         NaN   NaN\nlemmatization                                        NaN         NaN   NaN\nstemming(snow) + misspellings                        NaN         NaN   NaN\nlemmatization + misspellings                         NaN         NaN   NaN\nstemming(lanc) + misspellings                        NaN         NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing = ['just tokenization', 'stemming', 'lemmatization', 'stemming(snow) + misspellings',\n",
    "                                                        'lemmatization + misspellings', 'stemming(lanc) + misspellings']\n",
    "vectorizers = ['0 or 1, if the word exists', 'word counts', 'TFIDF']\n",
    "df_df = pd.DataFrame(columns=vectorizers, index=preprocessing)\n",
    "df_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97de009",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff2b02",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b689a1",
   "metadata": {},
   "source": [
    "##### Text cleaning function - basic + optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95f222",
   "metadata": {},
   "source": [
    "- contractions to full form\n",
    "- replace_emoticons with text\n",
    "- remove ticks and next symbol\n",
    "- remove url (http*)\n",
    "- remove hashtags (#)\n",
    "- remove mentions (@)\n",
    "- remove numbers\n",
    "- ignore case\n",
    "- ignore punctuation\n",
    "- remove extra spaces\n",
    "- remove stop words (optional)\n",
    "- remove misspelling (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b3ecc",
   "metadata": {},
   "source": [
    "### Just Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85e33",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking text into smaller units called tokens. In natural language processing (NLP) tasks, tokenization plays a crucial role for several reasons:\n",
    "\n",
    "1. **Simplification of Analysis**: Text, consisting of a continuous stream of characters, is difficult to analyze. Tokenization allows breaking down the text into words, phrases, or sentences, making further processing easier.\n",
    "\n",
    "2. **Standardization**: Tokenization helps standardize text, which is especially important for machine learning models. For example, different forms of a word (like \"run,\" \"running,\" \"ran\") can be reduced to a single form, improving the quality of analysis.\n",
    "\n",
    "3. **Noise Removal**: During tokenization, unnecessary characters (like punctuation) can be removed, allowing a focus on the meaningful parts of the text.\n",
    "\n",
    "4. **Dictionary Creation**: Many NLP algorithms require the creation of a token dictionary to represent the text in vector form. This is particularly important for tasks like text classification or sentiment analysis.\n",
    "\n",
    "5. **Data Preparation**: Tokenization is the first stage in preparing data for model training. Proper tokenization can significantly impact the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82bbee99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:08.366223Z",
     "start_time": "2024-08-03T22:00:07.798596Z"
    }
   },
   "outputs": [],
   "source": [
    "df_token = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2576",
   "metadata": {},
   "source": [
    "#### Applying the Text Cleaning Function (Basic Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f005d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:09.027537Z",
     "start_time": "2024-08-03T22:00:07.864380Z"
    }
   },
   "outputs": [],
   "source": [
    "df_token['tweets'] = df_token.apply(lambda item: text_cleaninig.clean(item.tweets), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5044330c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:09.189344Z",
     "start_time": "2024-08-03T22:00:08.512803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspiration in all aspects fashion fitness ...     1\n1  apka apna awam ka channel frankline tv aam adm...     1\n2  beautiful album from the greatest unsung guita...     1\n3  good luck to rich riding for great project in ...     1\n4                  omg he kissed him crying with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspiration in all aspects fashion fitness ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omg he kissed him crying with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca2b81",
   "metadata": {},
   "source": [
    "> ### \"0 or 1, if the word exists\" for tweets and words (document-term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fd34f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:26.321125Z",
     "start_time": "2024-08-03T22:00:08.569662Z"
    }
   },
   "outputs": [],
   "source": [
    "df_token_exist = text_processing.word_exists(df_token, 'tweets')\n",
    "df_df['0 or 1, if the word exists'][0] = df_token_exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06159a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:26.326768Z",
     "start_time": "2024-08-03T22:00:25.582919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    aa  aah  aam  aamby  \\\ntweets                                                                    \nan inspiration in all aspects fashion fitness b...   0    0    0      0   \napka apna awam ka channel frankline tv aam admi...   0    0    1      0   \nbeautiful album from the greatest unsung guitar...   0    0    0      0   \ngood luck to rich riding for great project in t...   0    0    0      0   \nomg he kissed him crying with joy                    0    0    0      0   \n\n                                                    aando  aap  aaree  \\\ntweets                                                                  \nan inspiration in all aspects fashion fitness b...      0    0      0   \napka apna awam ka channel frankline tv aam admi...      0    0      0   \nbeautiful album from the greatest unsung guitar...      0    0      0   \ngood luck to rich riding for great project in t...      0    0      0   \nomg he kissed him crying with joy                       0    0      0   \n\n                                                    abbeydale  abbreviation  \\\ntweets                                                                        \nan inspiration in all aspects fashion fitness b...          0             0   \napka apna awam ka channel frankline tv aam admi...          0             0   \nbeautiful album from the greatest unsung guitar...          0             0   \ngood luck to rich riding for great project in t...          0             0   \nomg he kissed him crying with joy                           0             0   \n\n                                                    abc  ...  yr  yummy  yura  \\\ntweets                                                   ...                    \nan inspiration in all aspects fashion fitness b...    0  ...   0      0     0   \napka apna awam ka channel frankline tv aam admi...    0  ...   0      0     0   \nbeautiful album from the greatest unsung guitar...    0  ...   0      0     0   \ngood luck to rich riding for great project in t...    0  ...   0      0     0   \nomg he kissed him crying with joy                     0  ...   0      0     0   \n\n                                                    yuri  zabardast  zac  zcc  \\\ntweets                                                                          \nan inspiration in all aspects fashion fitness b...     0          0    0    0   \napka apna awam ka channel frankline tv aam admi...     0          0    0    0   \nbeautiful album from the greatest unsung guitar...     0          0    0    0   \ngood luck to rich riding for great project in t...     0          0    0    0   \nomg he kissed him crying with joy                      0          0    0    0   \n\n                                                    zero  zoo  zoos  \ntweets                                                               \nan inspiration in all aspects fashion fitness b...     0    0     0  \napka apna awam ka channel frankline tv aam admi...     0    0     0  \nbeautiful album from the greatest unsung guitar...     0    0     0  \ngood luck to rich riding for great project in t...     0    0     0  \nomg he kissed him crying with joy                      0    0     0  \n\n[5 rows x 6157 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aah</th>\n      <th>aam</th>\n      <th>aamby</th>\n      <th>aando</th>\n      <th>aap</th>\n      <th>aaree</th>\n      <th>abbeydale</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>yummy</th>\n      <th>yura</th>\n      <th>yuri</th>\n      <th>zabardast</th>\n      <th>zac</th>\n      <th>zcc</th>\n      <th>zero</th>\n      <th>zoo</th>\n      <th>zoos</th>\n    </tr>\n    <tr>\n      <th>tweets</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an inspiration in all aspects fashion fitness beauty and personality happy face or smiley kisses thefashionicon</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>apka apna awam ka channel frankline tv aam admi production please visit or likes share happy face or smiley fb page</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>beautiful album from the greatest unsung guitar genius of our time and i have met the great backstage</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>good luck to rich riding for great project in this sunday can you donate</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>omg he kissed him crying with joy</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 6157 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_exist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc87ed0",
   "metadata": {},
   "source": [
    "> ### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba87a7c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:35.667947Z",
     "start_time": "2024-08-03T22:00:25.653229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspects fashion fitness beauty and personality happy face or smiley kisses thefashionicon to amulya patnaik has been appointed new delhi police commissioner patnaik is a agmut cadre ips officer\n",
      "Columns: 6157 entries, aa to zoos\n",
      "dtypes: int64(6157)\n",
      "memory usage: 127.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_token_count = text_processing.word_count(df_token, 'tweets')\n",
    "df_token_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    aa  aah  aam  aamby  \\\ntweets                                                                    \nan inspiration in all aspects fashion fitness b...   0    0    0      0   \napka apna awam ka channel frankline tv aam admi...   0    0    1      0   \nbeautiful album from the greatest unsung guitar...   0    0    0      0   \ngood luck to rich riding for great project in t...   0    0    0      0   \nomg he kissed him crying with joy                    0    0    0      0   \n\n                                                    aando  aap  aaree  \\\ntweets                                                                  \nan inspiration in all aspects fashion fitness b...      0    0      0   \napka apna awam ka channel frankline tv aam admi...      0    0      0   \nbeautiful album from the greatest unsung guitar...      0    0      0   \ngood luck to rich riding for great project in t...      0    0      0   \nomg he kissed him crying with joy                       0    0      0   \n\n                                                    abbeydale  abbreviation  \\\ntweets                                                                        \nan inspiration in all aspects fashion fitness b...          0             0   \napka apna awam ka channel frankline tv aam admi...          0             0   \nbeautiful album from the greatest unsung guitar...          0             0   \ngood luck to rich riding for great project in t...          0             0   \nomg he kissed him crying with joy                           0             0   \n\n                                                    abc  ...  yr  yummy  yura  \\\ntweets                                                   ...                    \nan inspiration in all aspects fashion fitness b...    0  ...   0      0     0   \napka apna awam ka channel frankline tv aam admi...    0  ...   0      0     0   \nbeautiful album from the greatest unsung guitar...    0  ...   0      0     0   \ngood luck to rich riding for great project in t...    0  ...   0      0     0   \nomg he kissed him crying with joy                     0  ...   0      0     0   \n\n                                                    yuri  zabardast  zac  zcc  \\\ntweets                                                                          \nan inspiration in all aspects fashion fitness b...     0          0    0    0   \napka apna awam ka channel frankline tv aam admi...     0          0    0    0   \nbeautiful album from the greatest unsung guitar...     0          0    0    0   \ngood luck to rich riding for great project in t...     0          0    0    0   \nomg he kissed him crying with joy                      0          0    0    0   \n\n                                                    zero  zoo  zoos  \ntweets                                                               \nan inspiration in all aspects fashion fitness b...     0    0     0  \napka apna awam ka channel frankline tv aam admi...     0    0     0  \nbeautiful album from the greatest unsung guitar...     0    0     0  \ngood luck to rich riding for great project in t...     0    0     0  \nomg he kissed him crying with joy                      0    0     0  \n\n[5 rows x 6157 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aah</th>\n      <th>aam</th>\n      <th>aamby</th>\n      <th>aando</th>\n      <th>aap</th>\n      <th>aaree</th>\n      <th>abbeydale</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>yummy</th>\n      <th>yura</th>\n      <th>yuri</th>\n      <th>zabardast</th>\n      <th>zac</th>\n      <th>zcc</th>\n      <th>zero</th>\n      <th>zoo</th>\n      <th>zoos</th>\n    </tr>\n    <tr>\n      <th>tweets</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an inspiration in all aspects fashion fitness beauty and personality happy face or smiley kisses thefashionicon</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>apka apna awam ka channel frankline tv aam admi production please visit or likes share happy face or smiley fb page</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>beautiful album from the greatest unsung guitar genius of our time and i have met the great backstage</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>good luck to rich riding for great project in this sunday can you donate</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>omg he kissed him crying with joy</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 6157 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_count.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:35.761849Z",
     "start_time": "2024-08-03T22:00:35.664902Z"
    }
   },
   "id": "b162a8e8244386cd"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e27f904c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:35.805434Z",
     "start_time": "2024-08-03T22:00:35.714919Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][0] = df_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccfc81",
   "metadata": {},
   "source": [
    "> ### tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) is a statistical method used to evaluate the importance of a word in a document relative to an entire corpus of texts. It is widely applied in information retrieval, text analytics, and machine learning. The method consists of two main components: **TF** and **IDF**.\n",
    "\n",
    "## 1. Term Frequency (TF)\n",
    "**Term Frequency** (TF) measures how often a word appears in a document. The formula for calculating TF is as follows:\n",
    "TF(t, d) = N_{t,d} / N_{d}\n",
    "where:\n",
    "- N_{t,d} is the number of times the term t appears in document d,\n",
    "- N_{d} is the total number of words in document d.\n",
    "\n",
    "The higher the term frequency, the more significant it is considered in the context of that document.\n",
    "\n",
    "## 2. Inverse Document Frequency (IDF)\n",
    "**Inverse Document Frequency** (IDF) helps to reduce the weight of common words that may not carry significant information (e.g., prepositions or general terms). The formula for calculating IDF is as follows:\n",
    "IDF(t, D) = log (N_{D} / N_{d,t} + 1)\n",
    "where:\n",
    "- N_{D}  is the total number of documents in the corpus,\n",
    "- N_{d,t} is the number of documents containing the term t.\n",
    "\n",
    "Thus, if a term occurs in many documents, its IDF will be low, indicating its lesser significance.\n",
    "\n",
    "## 3. Combining TF and IDF\n",
    "The final TF-IDF value for term t in document d is calculated as the product of TF and IDF:\n",
    "TF-IDF(t, d, D) = TF(t, d) x IDF(t, D)\n",
    "\n",
    "This value indicates how important the term is in the document compared to other documents in the corpus. A high TF-IDF value suggests that the term appears frequently in this document but rarely in others.\n",
    "\n",
    "## Applications of TF-IDF\n",
    "- **Information Retrieval**: Used for ranking documents by relevance to a search query.\n",
    "- **Text Classification**: Helps in creating vector representations of documents for machine learning algorithms.\n",
    "- **Keyword Extraction**: Allows for identifying the most significant words from a text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f349737170b17e46"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f55db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:36.237436Z",
     "start_time": "2024-08-03T22:00:35.752120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspects fashion fitness beauty and personality happy face or smiley kisses thefashionicon to amulya patnaik has been appointed new delhi police commissioner patnaik is a agmut cadre ips officer\n",
      "Columns: 6157 entries, aa to zoos\n",
      "dtypes: float64(6157)\n",
      "memory usage: 127.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_token_tfidf = text_processing.tfidf(df_token, 'tweets')\n",
    "df_token_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     aa  aah      aam  aamby  \\\ntweets                                                                         \nan inspiration in all aspects fashion fitness b...  0.0  0.0  0.00000    0.0   \napka apna awam ka channel frankline tv aam admi...  0.0  0.0  0.25964    0.0   \nbeautiful album from the greatest unsung guitar...  0.0  0.0  0.00000    0.0   \ngood luck to rich riding for great project in t...  0.0  0.0  0.00000    0.0   \nomg he kissed him crying with joy                   0.0  0.0  0.00000    0.0   \n\n                                                    aando  aap  aaree  \\\ntweets                                                                  \nan inspiration in all aspects fashion fitness b...    0.0  0.0    0.0   \napka apna awam ka channel frankline tv aam admi...    0.0  0.0    0.0   \nbeautiful album from the greatest unsung guitar...    0.0  0.0    0.0   \ngood luck to rich riding for great project in t...    0.0  0.0    0.0   \nomg he kissed him crying with joy                     0.0  0.0    0.0   \n\n                                                    abbeydale  abbreviation  \\\ntweets                                                                        \nan inspiration in all aspects fashion fitness b...        0.0           0.0   \napka apna awam ka channel frankline tv aam admi...        0.0           0.0   \nbeautiful album from the greatest unsung guitar...        0.0           0.0   \ngood luck to rich riding for great project in t...        0.0           0.0   \nomg he kissed him crying with joy                         0.0           0.0   \n\n                                                    abc  ...   yr  yummy  \\\ntweets                                                   ...               \nan inspiration in all aspects fashion fitness b...  0.0  ...  0.0    0.0   \napka apna awam ka channel frankline tv aam admi...  0.0  ...  0.0    0.0   \nbeautiful album from the greatest unsung guitar...  0.0  ...  0.0    0.0   \ngood luck to rich riding for great project in t...  0.0  ...  0.0    0.0   \nomg he kissed him crying with joy                   0.0  ...  0.0    0.0   \n\n                                                    yura  yuri  zabardast  \\\ntweets                                                                      \nan inspiration in all aspects fashion fitness b...   0.0   0.0        0.0   \napka apna awam ka channel frankline tv aam admi...   0.0   0.0        0.0   \nbeautiful album from the greatest unsung guitar...   0.0   0.0        0.0   \ngood luck to rich riding for great project in t...   0.0   0.0        0.0   \nomg he kissed him crying with joy                    0.0   0.0        0.0   \n\n                                                    zac  zcc  zero  zoo  zoos  \ntweets                                                                         \nan inspiration in all aspects fashion fitness b...  0.0  0.0   0.0  0.0   0.0  \napka apna awam ka channel frankline tv aam admi...  0.0  0.0   0.0  0.0   0.0  \nbeautiful album from the greatest unsung guitar...  0.0  0.0   0.0  0.0   0.0  \ngood luck to rich riding for great project in t...  0.0  0.0   0.0  0.0   0.0  \nomg he kissed him crying with joy                   0.0  0.0   0.0  0.0   0.0  \n\n[5 rows x 6157 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>aah</th>\n      <th>aam</th>\n      <th>aamby</th>\n      <th>aando</th>\n      <th>aap</th>\n      <th>aaree</th>\n      <th>abbeydale</th>\n      <th>abbreviation</th>\n      <th>abc</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>yummy</th>\n      <th>yura</th>\n      <th>yuri</th>\n      <th>zabardast</th>\n      <th>zac</th>\n      <th>zcc</th>\n      <th>zero</th>\n      <th>zoo</th>\n      <th>zoos</th>\n    </tr>\n    <tr>\n      <th>tweets</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an inspiration in all aspects fashion fitness beauty and personality happy face or smiley kisses thefashionicon</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>apka apna awam ka channel frankline tv aam admi production please visit or likes share happy face or smiley fb page</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25964</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>beautiful album from the greatest unsung guitar genius of our time and i have met the great backstage</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>good luck to rich riding for great project in this sunday can you donate</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>omg he kissed him crying with joy</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 6157 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_tfidf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:36.492204Z",
     "start_time": "2024-08-03T22:00:36.220913Z"
    }
   },
   "id": "be1bf9ef7023d8ce"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c6b3e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:36.494528Z",
     "start_time": "2024-08-03T22:00:36.290702Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][0] = df_token_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671572a",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc34d80",
   "metadata": {},
   "source": [
    "A stemming algorithm is a computational procedure which reduces all words with the same rootâ€¦ to a common form, usually by stripping each word of its derivational and inflectional suffixes.The process of stemming is aimed at mapping for retrieval purposes, the stem need not be a linguistically correct lemma or root.\n",
    "**Errors in Stemming**:\n",
    "_Over-Stemming_: It occurs when two or more unrelated words result in the same stem.\n",
    "_Under-Stemming_: It occurs when two or more related words result in different stems."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Porter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716241ca1d661c08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Porter Stemmer**: one of the most commonly used stemmers, developed by M.F. Porter in 1980. Porterâ€™s stemmer consists of five different phases. These phases are applied sequentially. Within each phase, there are certain conventions for selecting rules. The entire porter algorithm is small and thus fast and simple. The drawback of this stemmer is that it supports only the English language, and the stem obtained may or may not be linguistically correct."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a955dbb4b4caedf8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspir in all aspect fashion fit beauti and...     1\n1  apka apna awam ka channel franklin tv aam admi...     1\n2  beauti album from the greatest unsung guitar g...     1\n3  good luck to rich ride for great project in th...     1\n4                       omg he kiss him cri with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in all aspect fashion fit beauti and...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apka apna awam ka channel franklin tv aam admi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich ride for great project in th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omg he kiss him cri with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "df_stemmed_porter = df.copy(deep=True)\n",
    "df_stemmed_porter['tweets'] = df_stemmed_porter.apply(lambda item: text_processing.stem_text(item.tweets, porter), axis=1)\n",
    "df_stemmed_porter.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:37.319020Z",
     "start_time": "2024-08-03T22:00:36.330582Z"
    }
   },
   "id": "addc004a5601c870"
  },
  {
   "cell_type": "markdown",
   "id": "c5011839",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Snowball Stemmer**: It is a small language-independent stemming algorithm developed by Martin Porter. It works by removing the affixes from the word and iteratively trying to find the most basic form of the word. Snowball Stemmer supports multiple languages and is faster than the Porter Stemmer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47a2134f521625d5"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11128b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:38.394817Z",
     "start_time": "2024-08-03T22:00:37.317109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspir in all aspect fashion fit beauti and...     1\n1  apka apna awam ka channel franklin tv aam admi...     1\n2  beauti album from the greatest unsung guitar g...     1\n3  good luck to rich ride for great project in th...     1\n4                       omg he kiss him cri with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in all aspect fashion fit beauti and...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apka apna awam ka channel franklin tv aam admi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich ride for great project in th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omg he kiss him cri with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "df_stemmed_snow = df.copy(deep=True)\n",
    "df_stemmed_snow['tweets'] = df_stemmed_snow.apply(lambda item: text_processing.stem_text(item.tweets, snowball), axis=1)\n",
    "df_stemmed_snow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc831afe",
   "metadata": {},
   "source": [
    "#### Lancaster"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Lancaster Stemmer**: It is a more aggressive stemmer that tries to remove as many affixes as possible. It is also known as the \"Paice/Husk Stemmer\". It is more aggressive than the Porter Stemmer and can remove more affixes than the Snowball Stemmer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4b4aa11d96d3a9"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa89d31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:39.793154Z",
     "start_time": "2024-08-03T22:00:38.380636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspir in al aspect fash fit beauty and per...     1\n1  apk apn awam ka channel franklin tv aam adm pr...     1\n2  beauty alb from the greatest unsung guit geni ...     1\n3  good luck to rich rid for gre project in thi s...     1\n4                       omg he kiss him cry with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in al aspect fash fit beauty and per...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apk apn awam ka channel franklin tv aam adm pr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty alb from the greatest unsung guit geni ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich rid for gre project in thi s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omg he kiss him cry with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "df_stemmed_lanc = df.copy(deep=True)\n",
    "df_stemmed_lanc['tweets'] = df_stemmed_lanc.apply(lambda item: text_processing.stem_text(item.tweets, lancaster), axis=1)\n",
    "df_stemmed_lanc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883beec",
   "metadata": {},
   "source": [
    "> ### Applying '0 or 1, if the word exists' for Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f4a22e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:45.866213Z",
     "start_time": "2024-08-03T22:00:39.694184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss thefashionicon to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4926 entries, aa to zoo\n",
      "dtypes: int64(4926)\n",
      "memory usage: 102.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_exist = text_processing.word_exists(df_stemmed_snow, 'tweets')\n",
    "df_stem_exist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aca1386f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:45.969173Z",
     "start_time": "2024-08-03T22:00:45.859857Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['0 or 1, if the word exists'][1] = df_stem_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507c8ca",
   "metadata": {},
   "source": [
    "> ### Applying 'word count' for Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ca364f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:51.039456Z",
     "start_time": "2024-08-03T22:00:45.906297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss thefashionicon to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4926 entries, aa to zoo\n",
      "dtypes: int64(4926)\n",
      "memory usage: 102.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_count = text_processing.word_count(df_stemmed_snow, 'tweets')\n",
    "df_stem_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f61381d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:51.108552Z",
     "start_time": "2024-08-03T22:00:51.034899Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][1] = df_stem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13a4a2",
   "metadata": {},
   "source": [
    "> ### Applying 'tfidf' for Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "717cdd94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:51.372089Z",
     "start_time": "2024-08-03T22:00:51.083045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss thefashionicon to amulya patnaik has been appoint new delhi polic commission patnaik is a agmut cadr ip offic\n",
      "Columns: 4926 entries, aa to zoo\n",
      "dtypes: float64(4926)\n",
      "memory usage: 102.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_tfidf = text_processing.tfidf(df_stemmed_snow, 'tweets')\n",
    "df_stem_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac256dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:51.464941Z",
     "start_time": "2024-08-03T22:00:51.372373Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][1] = df_stem_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fb149",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Lemmatization** is the process of converting a word into its base or dictionary form, known as a **lemma**. This method is widely used in natural language processing (NLP) to enhance text analysis and information retrieval. Lemmatization helps reduce the number of unique words in texts, making them easier to analyze.\n",
    "\n",
    "## 1. Difference from Stemming\n",
    "Lemmatization is often confused with stemming, but these are two different processes:\n",
    "- **Stemming**: Removes word endings to obtain the root form of a word but does not always return a grammatically correct form. For example, \"running\" may be reduced to \"run,\" but \"better\" could become \"better\" or \"good.\"\n",
    "- **Lemmatization**: Always returns the word in its correct grammatical form. For example, \"better\" will be transformed into \"good.\"\n",
    "\n",
    "## 2. Lemmatization Process\n",
    "The lemmatization process involves several steps:\n",
    "1. **Part of Speech Determination**: To correctly convert a word to its lemma, it is essential to know its part of speech (noun, verb, adjective, etc.).\n",
    "2. **Lexical Analysis**: Based on the part of speech and the context of the word, the algorithm determines its lemma.\n",
    "3. **Use of Dictionaries**: Lemmatizers often use dictionaries and morphological rules for word transformation.\n",
    "\n",
    "## 3. Examples of Lemmatization\n",
    "- **Verbs**:\n",
    "  - \"running\" â†’ \"run\"\n",
    "  - \"was\" â†’ \"be\"\n",
    "- **Nouns**:\n",
    "  - \"geese\" â†’ \"goose\"\n",
    "  - \"children\" â†’ \"child\"\n",
    "- **Adjectives**:\n",
    "  - \"better\" â†’ \"good\"\n",
    "  - \"best\" â†’ \"good\"\n",
    "\n",
    "## 4. Applications of Lemmatization\n",
    "Lemmatization is applied in various fields:\n",
    "- **Information Retrieval**: Simplifies searches by allowing the retrieval of documents containing different forms of a word.\n",
    "- **Sentiment Analysis**: Helps determine the emotional tone of the text using the base forms of words.\n",
    "- **Text Classification**: Reduces data dimensionality, improving classification quality."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54de0ae88fdba3b4"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebcd56b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:53.157989Z",
     "start_time": "2024-08-03T22:00:51.416031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspiration in all aspect fashion fitness b...     1\n1  apka apna awam ka channel frankline tv aam adm...     1\n2  beautiful album from the greatest unsung guita...     1\n3  good luck to rich riding for great project in ...     1\n4                     omg he kissed him cry with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspiration in all aspect fashion fitness b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omg he kissed him cry with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = df.copy(deep=True)\n",
    "df_lemmatized['tweets'] = df_lemmatized.apply(lambda item: text_processing.lem_text(item.tweets), axis=1)\n",
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df842bea",
   "metadata": {},
   "source": [
    "> ### Applying '0 or 1, if the word exists' for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eff4d610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:59.918620Z",
     "start_time": "2024-08-03T22:00:53.152755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss thefashionicon to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5631 entries, aa to zoo\n",
      "dtypes: int64(5631)\n",
      "memory usage: 116.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_exist = text_processing.word_exists(df_lemmatized, 'tweets')\n",
    "df_lem_exist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0468962f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:00:59.985881Z",
     "start_time": "2024-08-03T22:00:59.903758Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['0 or 1, if the word exists'][2] = df_lem_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0fad7",
   "metadata": {},
   "source": [
    "> ### Applying 'word count' for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5a4b76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:01:05.794925Z",
     "start_time": "2024-08-03T22:00:59.960865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss thefashionicon to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5631 entries, aa to zoo\n",
      "dtypes: int64(5631)\n",
      "memory usage: 116.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_count = text_processing.word_count(df_lemmatized, 'tweets')\n",
    "df_lem_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1c7ee61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:01:05.847868Z",
     "start_time": "2024-08-03T22:01:05.793085Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][2] = df_lem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7774d",
   "metadata": {},
   "source": [
    "> ### Applying 'tfidf' for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ea30e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:01:06.273521Z",
     "start_time": "2024-08-03T22:01:05.832856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss thefashionicon to amulya patnaik ha been appointed new delhi police commissioner patnaik is a agmut cadre ip officer\n",
      "Columns: 5631 entries, aa to zoo\n",
      "dtypes: float64(5631)\n",
      "memory usage: 116.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_tfidf = text_processing.tfidf(df_lemmatized, 'tweets')\n",
    "df_lem_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83c0dcc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:01:06.334043Z",
     "start_time": "2024-08-03T22:01:06.269813Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][2] = df_lem_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a431b",
   "metadata": {},
   "source": [
    "### Stemming(snowball) + misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0274883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:04.709557Z",
     "start_time": "2024-08-03T22:01:06.308911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspir in all aspect fashion fit beauti and...     1\n1  aka anna away ka channel franklin to am admit ...     1\n2  beauti album from the greatest unsung guitar g...     1\n3  good luck to rich ride for great project in th...     1\n4                        om he kiss him cri with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in all aspect fashion fit beauti and...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aka anna away ka channel franklin to am admit ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich ride for great project in th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>om he kiss him cri with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem_spell_snow = df.copy(deep=True)\n",
    "df_stem_spell_snow['tweets'] = df_stem_spell_snow.apply(lambda item: text_processing.stem_text(item.tweets, snowball, misspelling=True), axis=1)\n",
    "df_stem_spell_snow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eba810",
   "metadata": {},
   "source": [
    "> ### Applying '0 or 1, if the word exists' for 'Stemming(snowball) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b4883b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:09.664789Z",
     "start_time": "2024-08-03T22:13:04.707431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss to amulet has been appoint new deli polic commission is a gamut cadr is offic\n",
      "Columns: 3857 entries, aah to zoo\n",
      "dtypes: int64(3857)\n",
      "memory usage: 79.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_exist = text_processing.word_exists(df_stem_spell_snow, 'tweets')\n",
    "df_stem_spell_exist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ded213f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:09.733813Z",
     "start_time": "2024-08-03T22:13:09.661910Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['0 or 1, if the word exists'][3] = df_stem_spell_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7285c1",
   "metadata": {},
   "source": [
    "> ### Applying 'word count' for 'Stemming(snowball) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a84f8875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:13.590949Z",
     "start_time": "2024-08-03T22:13:09.706449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss to amulet has been appoint new deli polic commission is a gamut cadr is offic\n",
      "Columns: 3857 entries, aah to zoo\n",
      "dtypes: int64(3857)\n",
      "memory usage: 79.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_count = text_processing.word_count(df_stem_spell_snow, 'tweets')\n",
    "df_stem_spell_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b829744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:13.658866Z",
     "start_time": "2024-08-03T22:13:13.586955Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][3] = df_stem_spell_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80829d0d",
   "metadata": {},
   "source": [
    "> ### Applying 'tfidf' for 'Stemming(snowball) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea3b20d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:13.866332Z",
     "start_time": "2024-08-03T22:13:13.625256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in all aspect fashion fit beauti and person happi face or smiley kiss to amulet has been appoint new deli polic commission is a gamut cadr is offic\n",
      "Columns: 3857 entries, aah to zoo\n",
      "dtypes: float64(3857)\n",
      "memory usage: 79.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stem_spell_tfidf = text_processing.tfidf(df_stem_spell_snow, 'tweets')\n",
    "df_stem_spell_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b697c0a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:13:13.922973Z",
     "start_time": "2024-08-03T22:13:13.852748Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][3] = df_stem_spell_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e775c0c",
   "metadata": {},
   "source": [
    "### Lemmatization + misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7804edf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:24:54.204164Z",
     "start_time": "2024-08-03T22:13:13.896835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspiration in all aspect fashion fitness b...     1\n1  aka anna away ka channel franklin to am admit ...     1\n2  beautiful album from the greatest unsung guita...     1\n3  good luck to rich riding for great project in ...     1\n4                      om he kissed him cry with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspiration in all aspect fashion fitness b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aka anna away ka channel franklin to am admit ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>om he kissed him cry with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = df.copy(deep=True)\n",
    "df_lemmatized['tweets'] = df_lemmatized.apply(lambda item: text_processing.lem_text(item.tweets, misspelling=True), axis=1)\n",
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8bba2",
   "metadata": {},
   "source": [
    "> ### Applying '0 or 1, if the word exists' for 'Lemmatization + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc212736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:24:59.779808Z",
     "start_time": "2024-08-03T22:24:54.200637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss to amulet ha been appointed new deli police commissioner is a gamut cadre is officer\n",
      "Columns: 4588 entries, aah to zoo\n",
      "dtypes: int64(4588)\n",
      "memory usage: 95.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_exist = text_processing.word_exists(df_lemmatized, 'tweets')\n",
    "df_lem_spell_exist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "545f0240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:24:59.846534Z",
     "start_time": "2024-08-03T22:24:59.776176Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['0 or 1, if the word exists'][4] = df_lem_spell_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230ed30",
   "metadata": {},
   "source": [
    "> ### Applying 'word count' for 'Lemmatization + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8597d8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:25:04.200613Z",
     "start_time": "2024-08-03T22:24:59.813194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss to amulet ha been appointed new deli police commissioner is a gamut cadre is officer\n",
      "Columns: 4588 entries, aah to zoo\n",
      "dtypes: int64(4588)\n",
      "memory usage: 95.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_count = text_processing.word_count(df_lemmatized, 'tweets')\n",
    "df_lem_spell_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0d4fb9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:25:04.262955Z",
     "start_time": "2024-08-03T22:25:04.198294Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][4] = df_lem_spell_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507c7e8",
   "metadata": {},
   "source": [
    "> ### Applying 'tfidf' for 'Lemmatization + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc30511a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:25:04.472414Z",
     "start_time": "2024-08-03T22:25:04.235756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspiration in all aspect fashion fitness beauty and personality happy face or smiley kiss to amulet ha been appointed new deli police commissioner is a gamut cadre is officer\n",
      "Columns: 4588 entries, aah to zoo\n",
      "dtypes: float64(4588)\n",
      "memory usage: 95.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_lem_spell_tfidf = text_processing.tfidf(df_lemmatized, 'tweets')\n",
    "df_lem_spell_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "169321bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:25:04.537250Z",
     "start_time": "2024-08-03T22:25:04.468773Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][4] = df_lem_spell_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062bffc",
   "metadata": {},
   "source": [
    "### Other ideas of preprocessing - Stemming(lancaster) + misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6244847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:45.114336Z",
     "start_time": "2024-08-03T22:25:04.507372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              tweets  type\n0  an inspir in al aspect fash fit beauty and per...     1\n1  ak ann away ka channel franklin to am admit pr...     1\n2  beauty alb from the greatest unsung guit geni ...     1\n3  good luck to rich rid for gre project in thi s...     1\n4                        om he kiss him cry with joy     1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>an inspir in al aspect fash fit beauty and per...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ak ann away ka channel franklin to am admit pr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beauty alb from the greatest unsung guit geni ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good luck to rich rid for gre project in thi s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>om he kiss him cry with joy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem_spell_lanc = df.copy(deep=True)\n",
    "df_stem_spell_lanc['tweets'] = df_stem_spell_lanc.apply(lambda item: text_processing.stem_text(item.tweets, lancaster, misspelling=True), axis=1)\n",
    "df_stem_spell_lanc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c7aad",
   "metadata": {},
   "source": [
    "> ### Applying '0 or 1, if the word exists' for 'Stemming(lancaster) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1aa6339c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:49.562097Z",
     "start_time": "2024-08-03T22:36:45.111136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in al aspect fash fit beauty and person happy fac or smiley kiss to amulet has been appoint new del pol commit is a gamut cadr is off\n",
      "Columns: 3382 entries, aah to zoo\n",
      "dtypes: int64(3382)\n",
      "memory usage: 70.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_other_exist = text_processing.word_exists(df_stem_spell_lanc, 'tweets')\n",
    "df_other_exist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82d71b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:49.629410Z",
     "start_time": "2024-08-03T22:36:49.558687Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['0 or 1, if the word exists'][5] = df_other_exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2a3e7",
   "metadata": {},
   "source": [
    "> ### Applying 'word count' for 'Stemming(lancaster) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7f9b056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:52.765226Z",
     "start_time": "2024-08-03T22:36:49.597115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in al aspect fash fit beauty and person happy fac or smiley kiss to amulet has been appoint new del pol commit is a gamut cadr is off\n",
      "Columns: 3382 entries, aah to zoo\n",
      "dtypes: int64(3382)\n",
      "memory usage: 70.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_other_count = text_processing.word_count(df_stem_spell_lanc, 'tweets')\n",
    "df_other_count.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daca21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:52.831506Z",
     "start_time": "2024-08-03T22:36:52.762559Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['word counts'][5] = df_other_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11258b99",
   "metadata": {},
   "source": [
    "> ### Applying 'tfidf' for 'Stemming(lancaster) + misspellings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d7a23dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:53.003063Z",
     "start_time": "2024-08-03T22:36:52.802984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2715 entries, an inspir in al aspect fash fit beauty and person happy fac or smiley kiss to amulet has been appoint new del pol commit is a gamut cadr is off\n",
      "Columns: 3382 entries, aah to zoo\n",
      "dtypes: float64(3382)\n",
      "memory usage: 70.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_other_tfidf = text_processing.tfidf(df_stem_spell_lanc, 'tweets')\n",
    "df_other_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b48c4c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:36:53.067690Z",
     "start_time": "2024-08-03T22:36:53.001387Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df['TFIDF'][5] = df_other_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15751c5c",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cosine Similarity** is a measure used to evaluate the similarity between two vectors in space, based on the cosine of the angle between them. It is often applied in natural language processing and machine learning to compare texts, documents, or other objects represented as vectors.\n",
    "\n",
    "### Formula\n",
    "\n",
    "Cosine similarity is defined by the following formula:\n",
    "cosine\\_similarity}(A, B) = {A x B} / {|A| x |B|}\n",
    "where:\n",
    "- A and B are vectors,\n",
    "- A x B is the dot product of the vectors,\n",
    "- |A| and |B| are the norms (lengths) of the vectors A and B respectively.\n",
    "\n",
    "### Meaning\n",
    "\n",
    "Cosine similarity takes values from -1 to 1:\n",
    "- 1 means the vectors are identical (angle 0Â°),\n",
    "- 0 means the vectors are orthogonal (angle 90Â°),\n",
    "- -1 means the vectors are opposite (angle 180Â°).\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Text Processing**: Used to determine the similarity between documents or queries.\n",
    "2. **Recommendation Systems**: Helps find similar products or movies.\n",
    "3. **Classification**: Used in machine learning algorithms for grouping similar objects."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e98aea598c5f484"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d57ad0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:48.278258Z",
     "start_time": "2024-08-03T22:36:53.038508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       0 or 1, if the word exists + just tokenization  \\\n1   6. thanks happy - 316. thanks b happy\\n6. than...   \n2   6. thanks happy - 458. thanks happy\\n6. thanks...   \n3   6. thanks happy - 260. thanks happy\\n6. thanks...   \n4   16. share the love thanks for being top new fo...   \n5   60. thanks for the recent follow happy to conn...   \n6   6. thanks happy - 406. thanks happy\\n6. thanks...   \n7   60. thanks for the recent follow happy to conn...   \n8   60. thanks for the recent follow happy to conn...   \n9   60. thanks for the recent follow happy to conn...   \n10  26. hey thanks for being top new followers thi...   \n\n                0 or 1, if the word exists + stemming  \\\n1   6. thank happi - 458. thank happi\\n6. thanks h...   \n2   60. thank for the recent follow happi to conne...   \n3   26. hey thank for be top new follow this week ...   \n4   60. thank for the recent follow happi to conne...   \n5   6. thank happi - 260. thank happi\\n6. thanks h...   \n6   60. thank for the recent follow happi to conne...   \n7   60. thank for the recent follow happi to conne...   \n8   16. share the love thank for be top new follow...   \n9   6. thank happi - 316. thank b happi\\n6. thanks...   \n10  6. thank happi - 406. thank happi\\n6. thanks h...   \n\n           0 or 1, if the word exists + lemmatization  \\\n1   60. thanks for the recent follow happy to conn...   \n2   6. thanks happy - 406. thanks happy\\n6. thanks...   \n3   16. share the love thanks for being top new fo...   \n4   60. thanks for the recent follow happy to conn...   \n5   60. thanks for the recent follow happy to conn...   \n6   60. thanks for the recent follow happy to conn...   \n7   6. thanks happy - 458. thanks happy\\n6. thanks...   \n8   26. hey thanks for being top new follower this...   \n9   6. thanks happy - 316. thanks b happy\\n6. than...   \n10  6. thanks happy - 260. thanks happy\\n6. thanks...   \n\n   0 or 1, if the word exists + stemming(snow) + misspellings  \\\n1   16. share the love thank for be top new follow...           \n2   260. thank happi - 406. thank happi\\n260. Than...           \n3   46. thank for the recent follow happi to conne...           \n4   6. thank happi - 458. thank happi\\n6. thanks h...           \n5   6. thank happi - 260. thank happi\\n6. thanks h...           \n6   260. thank happi - 316. thank b happi\\n260. Th...           \n7   6. thank happi - 406. thank happi\\n6. thanks h...           \n8   260. thank happi - 458. thank happi\\n260. Than...           \n9   26. hey thank for be top new follow this week ...           \n10  6. thank happi - 316. thank b happi\\n6. thanks...           \n\n   0 or 1, if the word exists + lemmatization + misspellings  \\\n1   6. thanks happy - 316. thanks b happy\\n6. than...          \n2   46. thanks for the recent follow happy to conn...          \n3   260. thanks happy - 458. thanks happy\\n260. Th...          \n4   6. thanks happy - 260. thanks happy\\n6. thanks...          \n5   16. share the love thanks for being top new fo...          \n6   6. thanks happy - 406. thanks happy\\n6. thanks...          \n7   26. hey thanks for being top new follower this...          \n8   260. thanks happy - 316. thanks b happy\\n260. ...          \n9   260. thanks happy - 406. thanks happy\\n260. Th...          \n10  6. thanks happy - 458. thanks happy\\n6. thanks...          \n\n   0 or 1, if the word exists + stemming(lanc) + misspellings  \\\n1   16. shar the lov thank for being top new follo...           \n2   260. thank happy - 406. thank happy\\n260. Than...           \n3   260. thank happy - 458. thank happy\\n260. Than...           \n4   6. thank happy - 458. thank happy\\n6. thanks h...           \n5   6. thank happy - 260. thank happy\\n6. thanks h...           \n6   6. thank happy - 406. thank happy\\n6. thanks h...           \n7   46. thank for the rec follow happy to connect ...           \n8   260. thank happy - 316. thank b happy\\n260. Th...           \n9   26. hey thank for being top new follow thi wee...           \n10  6. thank happy - 316. thank b happy\\n6. thanks...           \n\n                      word counts + just tokenization  \\\n1   6. thanks happy - 316. thanks b happy\\n6. than...   \n2   6. thanks happy - 458. thanks happy\\n6. thanks...   \n3   6. thanks happy - 260. thanks happy\\n6. thanks...   \n4   16. share the love thanks for being top new fo...   \n5   60. thanks for the recent follow happy to conn...   \n6   6. thanks happy - 406. thanks happy\\n6. thanks...   \n7   60. thanks for the recent follow happy to conn...   \n8   60. thanks for the recent follow happy to conn...   \n9   60. thanks for the recent follow happy to conn...   \n10  26. hey thanks for being top new followers thi...   \n\n                               word counts + stemming  \\\n1   6. thank happi - 458. thank happi\\n6. thanks h...   \n2   60. thank for the recent follow happi to conne...   \n3   26. hey thank for be top new follow this week ...   \n4   60. thank for the recent follow happi to conne...   \n5   6. thank happi - 260. thank happi\\n6. thanks h...   \n6   60. thank for the recent follow happi to conne...   \n7   60. thank for the recent follow happi to conne...   \n8   16. share the love thank for be top new follow...   \n9   6. thank happi - 316. thank b happi\\n6. thanks...   \n10  6. thank happi - 406. thank happi\\n6. thanks h...   \n\n                          word counts + lemmatization  \\\n1   60. thanks for the recent follow happy to conn...   \n2   6. thanks happy - 406. thanks happy\\n6. thanks...   \n3   16. share the love thanks for being top new fo...   \n4   60. thanks for the recent follow happy to conn...   \n5   60. thanks for the recent follow happy to conn...   \n6   60. thanks for the recent follow happy to conn...   \n7   6. thanks happy - 458. thanks happy\\n6. thanks...   \n8   26. hey thanks for being top new follower this...   \n9   6. thanks happy - 316. thanks b happy\\n6. than...   \n10  6. thanks happy - 260. thanks happy\\n6. thanks...   \n\n          word counts + stemming(snow) + misspellings  \\\n1   16. share the love thank for be top new follow...   \n2   260. thank happi - 406. thank happi\\n260. Than...   \n3   46. thank for the recent follow happi to conne...   \n4   6. thank happi - 458. thank happi\\n6. thanks h...   \n5   6. thank happi - 260. thank happi\\n6. thanks h...   \n6   260. thank happi - 316. thank b happi\\n260. Th...   \n7   6. thank happi - 406. thank happi\\n6. thanks h...   \n8   260. thank happi - 458. thank happi\\n260. Than...   \n9   26. hey thank for be top new follow this week ...   \n10  6. thank happi - 316. thank b happi\\n6. thanks...   \n\n           word counts + lemmatization + misspellings  \\\n1   6. thanks happy - 316. thanks b happy\\n6. than...   \n2   46. thanks for the recent follow happy to conn...   \n3   260. thanks happy - 458. thanks happy\\n260. Th...   \n4   6. thanks happy - 260. thanks happy\\n6. thanks...   \n5   16. share the love thanks for being top new fo...   \n6   6. thanks happy - 406. thanks happy\\n6. thanks...   \n7   26. hey thanks for being top new follower this...   \n8   260. thanks happy - 316. thanks b happy\\n260. ...   \n9   260. thanks happy - 406. thanks happy\\n260. Th...   \n10  6. thanks happy - 458. thanks happy\\n6. thanks...   \n\n          word counts + stemming(lanc) + misspellings  \\\n1   16. shar the lov thank for being top new follo...   \n2   260. thank happy - 406. thank happy\\n260. Than...   \n3   260. thank happy - 458. thank happy\\n260. Than...   \n4   6. thank happy - 458. thank happy\\n6. thanks h...   \n5   6. thank happy - 260. thank happy\\n6. thanks h...   \n6   6. thank happy - 406. thank happy\\n6. thanks h...   \n7   46. thank for the rec follow happy to connect ...   \n8   260. thank happy - 316. thank b happy\\n260. Th...   \n9   26. hey thank for being top new follow thi wee...   \n10  6. thank happy - 316. thank b happy\\n6. thanks...   \n\n                            TFIDF + just tokenization  \\\n1   60. thanks for the recent follow happy to conn...   \n2   60. thanks for the recent follow happy to conn...   \n3   221. thanks for the recent follow happy to con...   \n4   60. thanks for the recent follow happy to conn...   \n5   221. thanks for the recent follow happy to con...   \n6   60. thanks for the recent follow happy to conn...   \n7   221. thanks for the recent follow happy to con...   \n8   16. share the love thanks for being top new fo...   \n9   60. thanks for the recent follow happy to conn...   \n10  221. thanks for the recent follow happy to con...   \n\n                                     TFIDF + stemming  \\\n1   6. thank happi - 260. thank happi\\n6. thanks h...   \n2   60. thank for the recent follow happi to conne...   \n3   6. thank happi - 316. thank b happi\\n6. thanks...   \n4   60. thank for the recent follow happi to conne...   \n5   60. thank for the recent follow happi to conne...   \n6   60. thank for the recent follow happi to conne...   \n7   60. thank for the recent follow happi to conne...   \n8   6. thank happi - 458. thank happi\\n6. thanks h...   \n9   6. thank happi - 406. thank happi\\n6. thanks h...   \n10  16. share the love thank for be top new follow...   \n\n                                TFIDF + lemmatization  \\\n1   60. thanks for the recent follow happy to conn...   \n2   221. thanks for the recent follow happy to con...   \n3   60. thanks for the recent follow happy to conn...   \n4   60. thanks for the recent follow happy to conn...   \n5   16. share the love thanks for being top new fo...   \n6   221. thanks for the recent follow happy to con...   \n7   60. thanks for the recent follow happy to conn...   \n8   60. thanks for the recent follow happy to conn...   \n9   221. thanks for the recent follow happy to con...   \n10  26. hey thanks for being top new follower this...   \n\n                TFIDF + stemming(snow) + misspellings  \\\n1   6. thank happi - 406. thank happi\\n6. thanks h...   \n2   6. thank happi - 260. thank happi\\n6. thanks h...   \n3   260. thank happi - 458. thank happi\\n260. Than...   \n4   26. hey thank for be top new follow this week ...   \n5   6. thank happi - 316. thank b happi\\n6. thanks...   \n6   6. thank happi - 458. thank happi\\n6. thanks h...   \n7   93. happi - 150. happi\\n93. Goodevening happy ...   \n8   260. thank happi - 406. thank happi\\n260. Than...   \n9   316. thank b happi - 406. thank happi\\n316. th...   \n10  260. thank happi - 316. thank b happi\\n260. Th...   \n\n                 TFIDF + lemmatization + misspellings  \\\n1   60. thanks for the recent follow happy to conn...   \n2   221. thanks for the recent follow happy to con...   \n3   60. thanks for the recent follow happy to conn...   \n4   221. thanks for the recent follow happy to con...   \n5   60. thanks for the recent follow happy to conn...   \n6   60. thanks for the recent follow happy to conn...   \n7   221. thanks for the recent follow happy to con...   \n8   93. happy - 150. happy\\n93. Goodevening happy ...   \n9   60. thanks for the recent follow happy to conn...   \n10  60. thanks for the recent follow happy to conn...   \n\n                TFIDF + stemming(lanc) + misspellings  \n1   6. thank happy - 406. thank happy\\n6. thanks h...  \n2   60. thank for the rec follow happy to connect ...  \n3   6. thank happy - 260. thank happy\\n6. thanks h...  \n4   46. thank for the rec follow happy to connect ...  \n5   60. thank for the rec follow happy to connect ...  \n6   6. thank happy - 316. thank b happy\\n6. thanks...  \n7   60. thank for the rec follow happy to connect ...  \n8   26. hey thank for being top new follow thi wee...  \n9   16. shar the lov thank for being top new follo...  \n10  6. thank happy - 458. thank happy\\n6. thanks h...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists + just tokenization</th>\n      <th>0 or 1, if the word exists + stemming</th>\n      <th>0 or 1, if the word exists + lemmatization</th>\n      <th>0 or 1, if the word exists + stemming(snow) + misspellings</th>\n      <th>0 or 1, if the word exists + lemmatization + misspellings</th>\n      <th>0 or 1, if the word exists + stemming(lanc) + misspellings</th>\n      <th>word counts + just tokenization</th>\n      <th>word counts + stemming</th>\n      <th>word counts + lemmatization</th>\n      <th>word counts + stemming(snow) + misspellings</th>\n      <th>word counts + lemmatization + misspellings</th>\n      <th>word counts + stemming(lanc) + misspellings</th>\n      <th>TFIDF + just tokenization</th>\n      <th>TFIDF + stemming</th>\n      <th>TFIDF + lemmatization</th>\n      <th>TFIDF + stemming(snow) + misspellings</th>\n      <th>TFIDF + lemmatization + misspellings</th>\n      <th>TFIDF + stemming(lanc) + misspellings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>16. share the love thank for be top new follow...</td>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>16. shar the lov thank for being top new follo...</td>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>16. share the love thank for be top new follow...</td>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>16. shar the lov thank for being top new follo...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happy - 406. thank happy\\n6. thanks h...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>260. thank happi - 406. thank happi\\n260. Than...</td>\n      <td>46. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happy - 406. thank happy\\n260. Than...</td>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>260. thank happi - 406. thank happi\\n260. Than...</td>\n      <td>46. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happy - 406. thank happy\\n260. Than...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>60. thank for the rec follow happy to connect ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>26. hey thank for be top new follow this week ...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>46. thank for the recent follow happi to conne...</td>\n      <td>260. thanks happy - 458. thanks happy\\n260. Th...</td>\n      <td>260. thank happy - 458. thank happy\\n260. Than...</td>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>26. hey thank for be top new follow this week ...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>46. thank for the recent follow happi to conne...</td>\n      <td>260. thanks happy - 458. thanks happy\\n260. Th...</td>\n      <td>260. thank happy - 458. thank happy\\n260. Than...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happi - 458. thank happi\\n260. Than...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happy - 260. thank happy\\n6. thanks h...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 458. thank happy\\n6. thanks h...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 458. thank happy\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>26. hey thank for be top new follow this week ...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>46. thank for the rec follow happy to connect ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>6. thank happy - 260. thank happy\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 260. thank happi\\n6. thanks h...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>6. thank happy - 260. thank happy\\n6. thanks h...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the rec follow happy to connect ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happi - 316. thank b happi\\n260. Th...</td>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 406. thank happy\\n6. thanks h...</td>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happi - 316. thank b happi\\n260. Th...</td>\n      <td>6. thanks happy - 406. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 406. thank happy\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happy - 316. thank b happy\\n6. thanks...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>26. hey thanks for being top new follower this...</td>\n      <td>46. thank for the rec follow happy to connect ...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>26. hey thanks for being top new follower this...</td>\n      <td>46. thank for the rec follow happy to connect ...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>60. thank for the recent follow happi to conne...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>93. happi - 150. happi\\n93. Goodevening happy ...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>60. thank for the rec follow happy to connect ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>16. share the love thank for be top new follow...</td>\n      <td>26. hey thanks for being top new follower this...</td>\n      <td>260. thank happi - 458. thank happi\\n260. Than...</td>\n      <td>260. thanks happy - 316. thanks b happy\\n260. ...</td>\n      <td>260. thank happy - 316. thank b happy\\n260. Th...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>16. share the love thank for be top new follow...</td>\n      <td>26. hey thanks for being top new follower this...</td>\n      <td>260. thank happi - 458. thank happi\\n260. Than...</td>\n      <td>260. thanks happy - 316. thanks b happy\\n260. ...</td>\n      <td>260. thank happy - 316. thank b happy\\n260. Th...</td>\n      <td>16. share the love thanks for being top new fo...</td>\n      <td>6. thank happi - 458. thank happi\\n6. thanks h...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>260. thank happi - 406. thank happi\\n260. Than...</td>\n      <td>93. happy - 150. happy\\n93. Goodevening happy ...</td>\n      <td>26. hey thank for being top new follow thi wee...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>26. hey thank for be top new follow this week ...</td>\n      <td>260. thanks happy - 406. thanks happy\\n260. Th...</td>\n      <td>26. hey thank for being top new follow thi wee...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>6. thanks happy - 316. thanks b happy\\n6. than...</td>\n      <td>26. hey thank for be top new follow this week ...</td>\n      <td>260. thanks happy - 406. thanks happy\\n260. Th...</td>\n      <td>26. hey thank for being top new follow thi wee...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>316. thank b happi - 406. thank happi\\n316. th...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>16. shar the lov thank for being top new follo...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>26. hey thanks for being top new followers thi...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 316. thank b happy\\n6. thanks...</td>\n      <td>26. hey thanks for being top new followers thi...</td>\n      <td>6. thank happi - 406. thank happi\\n6. thanks h...</td>\n      <td>6. thanks happy - 260. thanks happy\\n6. thanks...</td>\n      <td>6. thank happi - 316. thank b happi\\n6. thanks...</td>\n      <td>6. thanks happy - 458. thanks happy\\n6. thanks...</td>\n      <td>6. thank happy - 316. thank b happy\\n6. thanks...</td>\n      <td>221. thanks for the recent follow happy to con...</td>\n      <td>16. share the love thank for be top new follow...</td>\n      <td>26. hey thanks for being top new follower this...</td>\n      <td>260. thank happi - 316. thank b happi\\n260. Th...</td>\n      <td>60. thanks for the recent follow happy to conn...</td>\n      <td>6. thank happy - 458. thank happy\\n6. thanks h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = cosine_similarity.cosine_similarity(df, df_df)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "615205b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:48.359954Z",
     "start_time": "2024-08-03T22:41:48.275368Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res.to_csv('res/cos_sim.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e7f6c",
   "metadata": {},
   "source": [
    "###  The top10 similar pairs of tweets for different vectorizer and preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0930bb6",
   "metadata": {},
   "source": [
    "> ### vectorizer + preprocessor: tweets after preprocessing / tweets in original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b084c93e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:48.455091Z",
     "start_time": "2024-08-03T22:41:48.333547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 or 1, if the word exists + just tokenization\n",
      "\t1.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t2.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t3.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t4.\n",
      "16. share the love thanks for being top new followers this week happy want this - 621. share the love thanks for being top new followers this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t5.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t8.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t9.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t10.\n",
      "26. hey thanks for being top new followers this week much appreciated happy want this - 775. hey thanks for being top new followers this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\n",
      "\n",
      "0 or 1, if the word exists + stemming\n",
      "\t1.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t2.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 221. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t3.\n",
      "26. hey thank for be top new follow this week much appreci happi want this - 775. hey thank for be top new follow this week much appreci happi want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t4.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 322. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t5.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 482. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 293. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t8.\n",
      "16. share the love thank for be top new follow this week happi want this - 621. share the love thank for be top new follow this week happi want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t9.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t10.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\n",
      "\n",
      "0 or 1, if the word exists + lemmatization\n",
      "\t1.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t2.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "16. share the love thanks for being top new follower this week happy want this - 621. share the love thanks for being top new follower this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t4.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t5.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t8.\n",
      "26. hey thanks for being top new follower this week much appreciated happy want this - 775. hey thanks for being top new follower this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t9.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t10.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\n",
      "\n",
      "0 or 1, if the word exists + stemming(snow) + misspellings\n",
      "\t1.\n",
      "16. share the love thank for be top new follow this week happi want this - 621. share the love thank for be top new follow this week happi want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t2.\n",
      "260. thank happi - 406. thank happi\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "46. thank for the recent follow happi to connect happi have a great - 794. thank for the recent follow happi to connect happi have a great\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t4.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t5.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "260. thank happi - 316. thank b happi\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t7.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t8.\n",
      "260. thank happi - 458. thank happi\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t9.\n",
      "26. hey thank for be top new follow this week much appreci happi want this - 775. hey thank for be top new follow this week much appreci happi want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t10.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\n",
      "\n",
      "0 or 1, if the word exists + lemmatization + misspellings\n",
      "\t1.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t2.\n",
      "46. thanks for the recent follow happy to connect happy have a great - 794. thanks for the recent follow happy to connect happy have a great\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t3.\n",
      "260. thanks happy - 458. thanks happy\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t4.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t5.\n",
      "16. share the love thanks for being top new follower this week happy want this - 621. share the love thanks for being top new follower this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t6.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "26. hey thanks for being top new follower this week much appreciated happy want this - 775. hey thanks for being top new follower this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t8.\n",
      "260. thanks happy - 316. thanks b happy\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t9.\n",
      "260. thanks happy - 406. thanks happy\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t10.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\n",
      "\n",
      "0 or 1, if the word exists + stemming(lanc) + misspellings\n",
      "\t1.\n",
      "16. shar the lov thank for being top new follow thi week happy want thi - 621. shar the lov thank for being top new follow thi week happy want thi\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t2.\n",
      "260. thank happy - 406. thank happy\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "260. thank happy - 458. thank happy\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t4.\n",
      "6. thank happy - 458. thank happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t5.\n",
      "6. thank happy - 260. thank happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "6. thank happy - 406. thank happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "46. thank for the rec follow happy to connect happy hav a gre - 794. thank for the rec follow happy to connect happy hav a gre\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t8.\n",
      "260. thank happy - 316. thank b happy\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t9.\n",
      "26. hey thank for being top new follow thi week much apprecy happy want thi - 775. hey thank for being top new follow thi week much apprecy happy want thi\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t10.\n",
      "6. thank happy - 316. thank b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\n",
      "\n",
      "word counts + just tokenization\n",
      "\t1.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t2.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t3.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t4.\n",
      "16. share the love thanks for being top new followers this week happy want this - 621. share the love thanks for being top new followers this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t5.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t8.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t9.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t10.\n",
      "26. hey thanks for being top new followers this week much appreciated happy want this - 775. hey thanks for being top new followers this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\n",
      "\n",
      "word counts + stemming\n",
      "\t1.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t2.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 221. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t3.\n",
      "26. hey thank for be top new follow this week much appreci happi want this - 775. hey thank for be top new follow this week much appreci happi want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t4.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 322. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t5.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 482. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 293. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t8.\n",
      "16. share the love thank for be top new follow this week happi want this - 621. share the love thank for be top new follow this week happi want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t9.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t10.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\n",
      "\n",
      "word counts + lemmatization\n",
      "\t1.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t2.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "16. share the love thanks for being top new follower this week happy want this - 621. share the love thanks for being top new follower this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t4.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t5.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t8.\n",
      "26. hey thanks for being top new follower this week much appreciated happy want this - 775. hey thanks for being top new follower this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t9.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t10.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\n",
      "\n",
      "word counts + stemming(snow) + misspellings\n",
      "\t1.\n",
      "16. share the love thank for be top new follow this week happi want this - 621. share the love thank for be top new follow this week happi want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t2.\n",
      "260. thank happi - 406. thank happi\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "46. thank for the recent follow happi to connect happi have a great - 794. thank for the recent follow happi to connect happi have a great\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t4.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t5.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "260. thank happi - 316. thank b happi\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t7.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t8.\n",
      "260. thank happi - 458. thank happi\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t9.\n",
      "26. hey thank for be top new follow this week much appreci happi want this - 775. hey thank for be top new follow this week much appreci happi want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t10.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\n",
      "\n",
      "word counts + lemmatization + misspellings\n",
      "\t1.\n",
      "6. thanks happy - 316. thanks b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t2.\n",
      "46. thanks for the recent follow happy to connect happy have a great - 794. thanks for the recent follow happy to connect happy have a great\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t3.\n",
      "260. thanks happy - 458. thanks happy\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t4.\n",
      "6. thanks happy - 260. thanks happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t5.\n",
      "16. share the love thanks for being top new follower this week happy want this - 621. share the love thanks for being top new follower this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t6.\n",
      "6. thanks happy - 406. thanks happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "26. hey thanks for being top new follower this week much appreciated happy want this - 775. hey thanks for being top new follower this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t8.\n",
      "260. thanks happy - 316. thanks b happy\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t9.\n",
      "260. thanks happy - 406. thanks happy\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t10.\n",
      "6. thanks happy - 458. thanks happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\n",
      "\n",
      "word counts + stemming(lanc) + misspellings\n",
      "\t1.\n",
      "16. shar the lov thank for being top new follow thi week happy want thi - 621. shar the lov thank for being top new follow thi week happy want thi\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t2.\n",
      "260. thank happy - 406. thank happy\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t3.\n",
      "260. thank happy - 458. thank happy\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t4.\n",
      "6. thank happy - 458. thank happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t5.\n",
      "6. thank happy - 260. thank happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t6.\n",
      "6. thank happy - 406. thank happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t7.\n",
      "46. thank for the rec follow happy to connect happy hav a gre - 794. thank for the rec follow happy to connect happy hav a gre\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t8.\n",
      "260. thank happy - 316. thank b happy\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\t9.\n",
      "26. hey thank for being top new follow thi week much apprecy happy want thi - 775. hey thank for being top new follow thi week much apprecy happy want thi\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t10.\n",
      "6. thank happy - 316. thank b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\n",
      "\n",
      "TFIDF + just tokenization\n",
      "\t1.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t2.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t3.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t4.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 815. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 815. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "\t5.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t6.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 815. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 815. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "\t8.\n",
      "16. share the love thanks for being top new followers this week happy want this - 621. share the love thanks for being top new followers this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t9.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t10.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\n",
      "\n",
      "TFIDF + stemming\n",
      "\t1.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t2.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 322. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t3.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t4.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 482. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t5.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 221. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 293. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t7.\n",
      "60. thank for the recent follow happi to connect happi have a great thursday want this - 815. thank for the recent follow happi to connect happi have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 815. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "\t8.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t9.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t10.\n",
      "16. share the love thank for be top new follow this week happi want this - 621. share the love thank for be top new follow this week happi want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\n",
      "\n",
      "TFIDF + lemmatization\n",
      "\t1.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t2.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 482. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t3.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t4.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 815. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 815. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "\t5.\n",
      "16. share the love thanks for being top new follower this week happy want this - 621. share the love thanks for being top new follower this week happy want this\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t6.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 322. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t7.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 221. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t8.\n",
      "60. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t9.\n",
      "221. thanks for the recent follow happy to connect happy have a great thursday want this - 293. thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t10.\n",
      "26. hey thanks for being top new follower this week much appreciated happy want this - 775. hey thanks for being top new follower this week much appreciated happy want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\n",
      "\n",
      "TFIDF + stemming(snow) + misspellings\n",
      "\t1.\n",
      "6. thank happi - 406. thank happi\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t2.\n",
      "6. thank happi - 260. thank happi\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t3.\n",
      "260. thank happi - 458. thank happi\n",
      "260. Thanks happy - 458. thanks! happy\n",
      "\t4.\n",
      "26. hey thank for be top new follow this week much appreci happi want this - 775. hey thank for be top new follow this week much appreci happi want this\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t5.\n",
      "6. thank happi - 316. thank b happi\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t6.\n",
      "6. thank happi - 458. thank happi\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\t7.\n",
      "93. happi - 150. happi\n",
      "93. Goodevening happy - 150. Mythbuster/20 happy\n",
      "\t8.\n",
      "260. thank happi - 406. thank happi\n",
      "260. Thanks happy - 406. .Thanks happy\n",
      "\t9.\n",
      "316. thank b happi - 406. thank happi\n",
      "316. thanks b happy - 406. .Thanks happy\n",
      "\t10.\n",
      "260. thank happi - 316. thank b happi\n",
      "260. Thanks happy - 316. thanks b happy\n",
      "\n",
      "\n",
      "TFIDF + lemmatization + misspellings\n",
      "\t1.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 221. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t2.\n",
      "221. thanks for the recent follow happy to connect happy have a great want this - 293. thanks for the recent follow happy to connect happy have a great want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t3.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 821. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 821. Thanks for the recent follow Happy to connect happy  have a great Wednesday. Want this\n",
      "\t4.\n",
      "221. thanks for the recent follow happy to connect happy have a great want this - 482. thanks for the recent follow happy to connect happy have a great want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t5.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 293. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t6.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 482. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 482. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this ?\n",
      "\t7.\n",
      "221. thanks for the recent follow happy to connect happy have a great want this - 322. thanks for the recent follow happy to connect happy have a great want this\n",
      "221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t8.\n",
      "93. happy - 150. happy\n",
      "93. Goodevening happy - 150. Mythbuster/20 happy\n",
      "\t9.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 322. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t10.\n",
      "60. thanks for the recent follow happy to connect happy have a great want this - 815. thanks for the recent follow happy to connect happy have a great want this\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 815. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this?\n",
      "\n",
      "\n",
      "TFIDF + stemming(lanc) + misspellings\n",
      "\t1.\n",
      "6. thank happy - 406. thank happy\n",
      "6. thanks happy - 406. .Thanks happy\n",
      "\t2.\n",
      "60. thank for the rec follow happy to connect happy hav a gre want thi - 293. thank for the rec follow happy to connect happy hav a gre want thi\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 293. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\n",
      "\t3.\n",
      "6. thank happy - 260. thank happy\n",
      "6. thanks happy - 260. Thanks happy\n",
      "\t4.\n",
      "46. thank for the rec follow happy to connect happy hav a gre - 794. thank for the rec follow happy to connect happy hav a gre\n",
      "46. Thanks for the recent follow Happy to connect happy  have a great Thursday. - 794. Thanks for the recent follow Happy to connect happy  have a great Wednesday.\n",
      "\t5.\n",
      "60. thank for the rec follow happy to connect happy hav a gre want thi - 221. thank for the rec follow happy to connect happy hav a gre want thi\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 221. Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\n",
      "\t6.\n",
      "6. thank happy - 316. thank b happy\n",
      "6. thanks happy - 316. thanks b happy\n",
      "\t7.\n",
      "60. thank for the rec follow happy to connect happy hav a gre want thi - 322. thank for the rec follow happy to connect happy hav a gre want thi\n",
      "60. Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? - 322. Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this ?\n",
      "\t8.\n",
      "26. hey thank for being top new follow thi week much apprecy happy want thi - 775. hey thank for being top new follow thi week much apprecy happy want thi\n",
      "26. Hey thanks for being top new followers this week! Much appreciated happy  Want this ? - 775. Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\n",
      "\t9.\n",
      "16. shar the lov thank for being top new follow thi week happy want thi - 621. shar the lov thank for being top new follow thi week happy want thi\n",
      "16. Share the love: thanks for being top new followers this week happy  Want this? - 621. Share the love:thanks for being top new followers this week happy   Want this\n",
      "\t10.\n",
      "6. thank happy - 458. thank happy\n",
      "6. thanks happy - 458. thanks! happy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity.print_cossim(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed302e",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19d6d3",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes - base classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Gaussian Naive Bayes** is a classification algorithm based on Bayes' theorem, which assumes that features (or attributes) are independent of each other. This method is particularly effective for classification tasks with continuous features, as it uses a normal (Gaussian) distribution to model the feature values.\n",
    "\n",
    "#### Key Principles\n",
    "1. **Bayes' Theorem**:\n",
    " P(C|X) = {P(X|C) x P(C)} / P(X)\n",
    "   where:\n",
    "   - P(C|X) is the posterior probability of class C given features X,\n",
    "   - P(X|C) is the likelihood of features X given class C,\n",
    "   - P(C) is the prior probability of class C,\n",
    "   - P(X) is the overall probability of features X.\n",
    "\n",
    "2. **Independence Assumption**:\n",
    "   The algorithm assumes that all features are independent of each other, simplifying calculations. Thus, for a multidimensional feature space, we can write:\n",
    "   P(X|C) = P(X_1|C) x P(X_2|C) x ... x P(X_n|C)\n",
    "\n",
    "3. **Gaussian Distribution**:\n",
    "   For each feature, the algorithm assumes that its values are distributed according to a normal (Gaussian) distribution. For each class C, the mean \\mu and standard deviation \\sigma are computed:\n",
    "   P(X_i|C) = 1 / {sqrt{2\\pi\\sigma^2}} x exp(-{(X_i - \\mu)^2} / {2 x \\sigma^2})\n",
    "\n",
    "#### Training Process\n",
    "1. **Data Collection**: Training data with known class labels is collected.\n",
    "2. **Parameter Calculation**:\n",
    "   - For each class C, the mean and standard deviation for each feature are calculated.\n",
    "3. **Prior Probabilities**: Prior probabilities for each class are computed based on the frequency of classes in the training set.\n",
    "\n",
    "#### Classification Process\n",
    "1. For a new instance X, the probability of belonging to each class C is calculated:\n",
    "   P(C|X):  P(C) x P(X|C)\n",
    "2. The class with the highest probability is selected as the predicted class.\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Simplicity of implementation and interpretation.\n",
    "- Fast operation, especially on large datasets.\n",
    "- Works well with high-dimensional data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- The independence assumption may not always hold, which can reduce accuracy.\n",
    "- Sensitive to outliers and noise in the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da1882fe223ee785"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c120c65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:48.657505Z",
     "start_time": "2024-08-03T22:41:48.382802Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1235e38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:53.174099Z",
     "start_time": "2024-08-03T22:41:48.477734Z"
    }
   },
   "outputs": [],
   "source": [
    "df_res = machine_learning.model_preprocessing(clf, df, df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77d48116",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:53.226020Z",
     "start_time": "2024-08-03T22:41:53.174337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              0 or 1, if the word exists word counts     TFIDF\njust tokenization                               0.734807    0.734807  0.707182\nstemming                                         0.73849     0.73849  0.720074\nlemmatization                                    0.73849     0.73849  0.709024\nstemming(snow) + misspellings                   0.734807    0.734807  0.710866\nlemmatization + misspellings                    0.729282    0.729282  0.697974\nstemming(lanc) + misspellings                   0.767956    0.767956  0.718232",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.734807</td>\n      <td>0.734807</td>\n      <td>0.707182</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.73849</td>\n      <td>0.73849</td>\n      <td>0.720074</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.73849</td>\n      <td>0.73849</td>\n      <td>0.709024</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.734807</td>\n      <td>0.734807</td>\n      <td>0.710866</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.729282</td>\n      <td>0.729282</td>\n      <td>0.697974</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.767956</td>\n      <td>0.767956</td>\n      <td>0.718232</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f63fc",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Logistic Regression** is a statistical method used for binary classification. It models the probability that an object belongs to a particular class based on one or more predictors (features). This method is often applied in tasks where the outcome is a binary value (e.g., \"yes\" or \"no,\" \"success\" or \"failure\").\n",
    "\n",
    "#### Key Principles\n",
    "1. **Model**:\n",
    "   Logistic regression uses the logistic function (or sigmoid function) to transform a linear combination of input features into a probability. The formula for the logistic function is as follows:\n",
    "   P(Y=1|X) = 1 / {1 + e^{-(b_0 + b_1 x X_1 + b_2 x X_2 + ... + b_n x X_n)}}\n",
    "   where:\n",
    "   - P(Y=1|X) is the probability that the target variable Y equals 1 (belongs to the positive class),\n",
    "   - b_0 is the intercept,\n",
    "   - b_1, b_2, ..., b_n are the coefficients (model parameters),\n",
    "   - X_1, X_2, ..., X_n are the predictors.\n",
    "\n",
    "2. **Model Training**:\n",
    "   To train the logistic regression model, the maximum likelihood estimation method is used. The goal is to find values for the coefficients b that maximize the probability of the observed data.\n",
    "\n",
    "3. **Prediction**:\n",
    "   After training the model, to predict the class of a new object, the probability is calculated, and if it exceeds a given threshold (usually 0.5), the object is classified as the positive class.\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Simplicity of interpretation of results.\n",
    "- Effectiveness when there is a linear relationship between features and the target variable.\n",
    "- Speed of computation and training.\n",
    "\n",
    "**Disadvantages**:\n",
    "- The assumption of a linear relationship between predictors and the log-odds may not always hold true.\n",
    "- Sensitivity to outliers and multicollinearity (high correlation between independent variables)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5891d6e6681fcd44"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1d25146",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:53.296629Z",
     "start_time": "2024-08-03T22:41:53.220060Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d4a24",
   "metadata": {},
   "source": [
    "#### Searching for best Logistic Regression params with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76bc6b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:41:53.396599Z",
     "start_time": "2024-08-03T22:41:53.267969Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty' : ['l1','l2'], \n",
    "    'C'       : [1., 10., 100.],\n",
    "    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57bbb974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:58:26.833392Z",
     "start_time": "2024-08-03T22:41:53.302144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just tokenization, 0 or 1, if the word exists:\n",
      "accuracy: 0.934438,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "just tokenization, word counts:\n",
      "accuracy: 0.934438,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "just tokenization, TFIDF:\n",
      "accuracy: 0.935175,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming, 0 or 1, if the word exists:\n",
      "accuracy: 0.934070,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "stemming, word counts:\n",
      "accuracy: 0.934070,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "stemming, TFIDF:\n",
      "accuracy: 0.935175,\n",
      "params = {'C': 100.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization, 0 or 1, if the word exists:\n",
      "accuracy: 0.932965,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization, word counts:\n",
      "accuracy: 0.932965,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization, TFIDF:\n",
      "accuracy: 0.934807,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(snow) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.935543,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(snow) + misspellings, word counts:\n",
      "accuracy: 0.935543,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(snow) + misspellings, TFIDF:\n",
      "accuracy: 0.938490,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.934438,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization + misspellings, word counts:\n",
      "accuracy: 0.934438,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "lemmatization + misspellings, TFIDF:\n",
      "accuracy: 0.936280,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(lanc) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.936280,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(lanc) + misspellings, word counts:\n",
      "accuracy: 0.936280,\n",
      "params = {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "stemming(lanc) + misspellings, TFIDF:\n",
      "accuracy: 0.938858,\n",
      "params = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "machine_learning.grid_search_all(clf, df, df_df, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634b58f",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Decision Tree** is a machine learning method used for both classification and regression. It represents a model that makes decisions based on a sequence of questions asked about the features of the input data. Each \"branch\" of the tree corresponds to a choice based on the value of a feature, and each \"leaf\" (terminal node) represents a prediction.\n",
    "\n",
    "#### Key Principles\n",
    "1. **Tree Structure**:\n",
    "   - **Root**: The starting node of the tree that contains all the data.\n",
    "   - **Internal Nodes**: Nodes representing questions about features that split the data into subsets.\n",
    "   - **Leaves**: Terminal nodes representing predictions (classes or values).\n",
    "\n",
    "2. **Building Algorithm**:\n",
    "   - **Feature Selection**: For each node in the tree, a feature must be chosen that best splits the data. Various criteria can be used for this, such as:\n",
    "     - **Gini Index**: A measure of impurity in the node, which minimizes the probability of misclassification.\n",
    "     - **Entropy Criterion**: Measures the uncertainty in the node. The lower the entropy, the more homogeneous the data.\n",
    "     - **Mean Squared Error**: Used in regression tasks to minimize the deviations of predicted values from actual values.\n",
    "\n",
    "3. **Pruning the Tree**:\n",
    "   - To prevent overfitting, the tree may be pruned. This is done by removing some nodes that do not significantly improve predictions, making the model more general.\n",
    "\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Simplicity of interpretation and visualization.\n",
    "- Does not require preprocessing of data, such as normalization or standardization.\n",
    "- Ability to work with both numerical and categorical data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Prone to overfitting, especially with deep trees.\n",
    "- Sensitive to small changes in the data, which can lead to significant changes in the structure of the tree.\n",
    "- Limited ability to model complex dependencies."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d49b5438e200486"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "549e873e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-03T22:58:26.872748Z",
     "start_time": "2024-08-03T22:58:26.828982Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf95bf",
   "metadata": {},
   "source": [
    "#### Searching for best Decision Tree params with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ddab8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T22:58:26.937996Z",
     "start_time": "2024-08-03T22:58:26.874149Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = [2, 4, 8, 16, 32]\n",
    "\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': max_depths,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c49f623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T23:04:31.232584Z",
     "start_time": "2024-08-03T22:58:26.920014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just tokenization, 0 or 1, if the word exists:\n",
      "accuracy: 0.925599,\n",
      "params = {'criterion': 'entropy', 'max_depth': 16}\n",
      "just tokenization, word counts:\n",
      "accuracy: 0.925967,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "just tokenization, TFIDF:\n",
      "accuracy: 0.929282,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "stemming, 0 or 1, if the word exists:\n",
      "accuracy: 0.924862,\n",
      "params = {'criterion': 'gini', 'max_depth': 32}\n",
      "stemming, word counts:\n",
      "accuracy: 0.925230,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "stemming, TFIDF:\n",
      "accuracy: 0.926335,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "lemmatization, 0 or 1, if the word exists:\n",
      "accuracy: 0.925599,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "lemmatization, word counts:\n",
      "accuracy: 0.925230,\n",
      "params = {'criterion': 'gini', 'max_depth': 16}\n",
      "lemmatization, TFIDF:\n",
      "accuracy: 0.926335,\n",
      "params = {'criterion': 'entropy', 'max_depth': 16}\n",
      "stemming(snow) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.923757,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "stemming(snow) + misspellings, word counts:\n",
      "accuracy: 0.925230,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "stemming(snow) + misspellings, TFIDF:\n",
      "accuracy: 0.924862,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "lemmatization + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.924125,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "lemmatization + misspellings, word counts:\n",
      "accuracy: 0.924125,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "lemmatization + misspellings, TFIDF:\n",
      "accuracy: 0.923757,\n",
      "params = {'criterion': 'entropy', 'max_depth': 16}\n",
      "stemming(lanc) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.925599,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "stemming(lanc) + misspellings, word counts:\n",
      "accuracy: 0.925599,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n",
      "stemming(lanc) + misspellings, TFIDF:\n",
      "accuracy: 0.924494,\n",
      "params = {'criterion': 'gini', 'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "machine_learning.grid_search_all(clf, df, df_df, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e01737",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Random Forest** is an ensemble machine learning method used for classification and regression. It builds multiple decision trees and combines their results to improve accuracy and prevent overfitting. This method is based on the concept of \"smart\" aggregation of multiple models to achieve a more reliable and stable outcome.\n",
    "\n",
    "#### Key Principles\n",
    "1. **Structure**:\n",
    "   - A Random Forest consists of many decision trees, each trained on a random subset of data. Each tree makes a decision, and the final result is obtained by voting (for classification) or averaging (for regression) the predictions of all trees.\n",
    "\n",
    "2. **Random Sampling**:\n",
    "   - For creating each tree, the bootstrap method is used, which includes randomly extracting a subset from the original dataset with replacement. This allows each tree to be trained on different data, increasing model diversity.\n",
    "\n",
    "3. **Random Feature Selection**:\n",
    "   - At each node split, a random subset of features is selected. This helps prevent overfitting and makes the model more robust to noise in the data.\n",
    "\n",
    "4. **Combining Results**:\n",
    "   - For classification, the final prediction is determined by voting, where the class that receives the most votes from the trees is considered the final prediction. For regression, the results are averaged.\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- High accuracy and robustness against overfitting.\n",
    "- Ability to handle large datasets with many features.\n",
    "- Resilience to outliers and noise in the data.\n",
    "- Ease of interpreting feature importance.\n",
    "\n",
    "**Disadvantages**:\n",
    "- High computational complexity during model training due to the large number of trees.\n",
    "- Lower interpretability compared to single decision trees.\n",
    "- Possibility of overfitting if the number of trees is too large or if they are too deep."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "255007b5ee40b91c"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f29893f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T23:04:31.276677Z",
     "start_time": "2024-08-03T23:04:31.230886Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2210c",
   "metadata": {},
   "source": [
    "#### Searching for best Random Forest params with Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "001ddcc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T23:04:31.303952Z",
     "start_time": "2024-08-03T23:04:31.263646Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = [2, 4, 8, 16, 32]\n",
    "n_estimators = [1000]\n",
    "\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': max_depths,\n",
    "    'n_estimators': n_estimators\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7199d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T01:13:12.767436Z",
     "start_time": "2024-08-03T23:04:31.303378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just tokenization, 0 or 1, if the word exists:\n",
      "accuracy: 0.934438,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
      "just tokenization, word counts:\n",
      "accuracy: 0.934438,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
      "just tokenization, TFIDF:\n",
      "accuracy: 0.933702,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming, 0 or 1, if the word exists:\n",
      "accuracy: 0.937753,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming, word counts:\n",
      "accuracy: 0.937753,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming, TFIDF:\n",
      "accuracy: 0.936648,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization, 0 or 1, if the word exists:\n",
      "accuracy: 0.937753,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization, word counts:\n",
      "accuracy: 0.937753,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization, TFIDF:\n",
      "accuracy: 0.936280,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(snow) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.939227,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(snow) + misspellings, word counts:\n",
      "accuracy: 0.939227,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(snow) + misspellings, TFIDF:\n",
      "accuracy: 0.936280,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.936280,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization + misspellings, word counts:\n",
      "accuracy: 0.936280,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "lemmatization + misspellings, TFIDF:\n",
      "accuracy: 0.934070,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(lanc) + misspellings, 0 or 1, if the word exists:\n",
      "accuracy: 0.938122,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(lanc) + misspellings, word counts:\n",
      "accuracy: 0.938122,\n",
      "params = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
      "stemming(lanc) + misspellings, TFIDF:\n",
      "accuracy: 0.937017,\n",
      "params = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "machine_learning.grid_search_all(clf, df, df_df, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638f769",
   "metadata": {},
   "source": [
    "### Choosing the best model and calculating final accuracies for all preprocessing and vectoring methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4eaa7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Logistic regression with params for best model according to gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e76e7879",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:13:15.087713Z",
     "start_time": "2024-08-04T01:13:12.740999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              0 or 1, if the word exists word counts     TFIDF\njust tokenization                                0.93186     0.93186  0.922652\nstemming                                         0.93186     0.93186  0.930018\nlemmatization                                    0.93186     0.93186  0.922652\nstemming(snow) + misspellings                   0.933702    0.933702  0.930018\nlemmatization + misspellings                     0.93186     0.93186  0.926335\nstemming(lanc) + misspellings                   0.935543    0.935543  0.928177",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.93186</td>\n      <td>0.93186</td>\n      <td>0.922652</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.93186</td>\n      <td>0.93186</td>\n      <td>0.930018</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.93186</td>\n      <td>0.93186</td>\n      <td>0.922652</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.933702</td>\n      <td>0.933702</td>\n      <td>0.930018</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.93186</td>\n      <td>0.93186</td>\n      <td>0.926335</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.935543</td>\n      <td>0.935543</td>\n      <td>0.928177</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "params_best = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "clf.set_params(**params_best)\n",
    "df_res_upd = machine_learning.model_preprocessing(clf, df, df_df)\n",
    "df_res_upd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448d285",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Decision Tree with params for best model according to gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a57f4b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:13:24.426295Z",
     "start_time": "2024-08-04T01:13:15.040312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              0 or 1, if the word exists word counts     TFIDF\njust tokenization                                0.92081    0.922652   0.92081\nstemming                                        0.913444    0.917127  0.913444\nlemmatization                                    0.92081    0.917127   0.92081\nstemming(snow) + misspellings                   0.911602    0.918969  0.911602\nlemmatization + misspellings                    0.917127    0.917127  0.917127\nstemming(lanc) + misspellings                   0.913444    0.909761  0.917127",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.92081</td>\n      <td>0.922652</td>\n      <td>0.92081</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.913444</td>\n      <td>0.917127</td>\n      <td>0.913444</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.92081</td>\n      <td>0.917127</td>\n      <td>0.92081</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.911602</td>\n      <td>0.918969</td>\n      <td>0.911602</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.917127</td>\n      <td>0.917127</td>\n      <td>0.917127</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.913444</td>\n      <td>0.909761</td>\n      <td>0.917127</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "params_best = {'criterion': 'gini', 'max_depth': 16}\n",
    "clf.set_params(**params_best)\n",
    "df_res_upd = machine_learning.model_preprocessing(clf, df, df_df)\n",
    "df_res_upd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cc6c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Random Forest with params for best model according to gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3f6c745",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.075611Z",
     "start_time": "2024-08-04T01:13:24.421627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              0 or 1, if the word exists word counts     TFIDF\njust tokenization                               0.928177    0.928177  0.924494\nstemming                                        0.935543    0.935543  0.928177\nlemmatization                                   0.935543    0.935543  0.930018\nstemming(snow) + misspellings                   0.937385    0.937385  0.928177\nlemmatization + misspellings                    0.930018    0.930018  0.922652\nstemming(lanc) + misspellings                   0.941068    0.941068   0.93186",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0 or 1, if the word exists</th>\n      <th>word counts</th>\n      <th>TFIDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.928177</td>\n      <td>0.928177</td>\n      <td>0.924494</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.935543</td>\n      <td>0.935543</td>\n      <td>0.928177</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.935543</td>\n      <td>0.935543</td>\n      <td>0.930018</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.937385</td>\n      <td>0.937385</td>\n      <td>0.928177</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.930018</td>\n      <td>0.930018</td>\n      <td>0.922652</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.941068</td>\n      <td>0.941068</td>\n      <td>0.93186</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "params_best = {'criterion': 'entropy', 'max_depth': 32, 'n_estimators': 1000}\n",
    "clf.set_params(**params_best)\n",
    "df_res_upd = machine_learning.model_preprocessing(clf, df, df_df)\n",
    "df_res_upd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46119c7c",
   "metadata": {},
   "source": [
    "----\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word2Vec** is a method for representing words as vectors, allowing machine learning models to effectively process textual data. Word2Vec uses neural networks to learn vector representations of words that capture semantic and syntactic relationships between them.\n",
    "\n",
    "#### Key Principles\n",
    "1. **Vector Representations**:\n",
    "   - Each word is represented as a multi-dimensional vector. These vectors allow models to compare words based on their meanings and context.\n",
    "      - \n",
    "2. **Algorithms**:\n",
    "   Word2Vec uses two main algorithms for training:\n",
    "   - **Continuous Bag of Words (CBOW)**: This method predicts the current word based on its context (the surrounding words). For example, given the sentence \"the cat sits on the mat,\" the model would try to predict the word \"sits\" using \"the,\" \"cat,\" \"on,\" and \"the mat\" as context.\n",
    "   - **Skip-gram**: This method, on the other hand, uses the current word to predict the context. So, if we have the word \"sits,\" the model will try to predict the words that surround it.\n",
    "\n",
    "3. **Training**:\n",
    "   - The model is trained on large volumes of textual data. During the training process, it optimizes the vectors so that words with similar meanings are located closer together in the vector space.\n",
    "\n",
    "4. **Semantic Relationships**:\n",
    "   - Word2Vec can capture semantic relationships between words. For instance, the vectors for \"king\" and \"queen\" will be close, as will the vectors for \"man\" and \"woman.\" This allows for operations like \"king - man + woman = queen.\"\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Efficiency: Allows for quick and effective processing of large volumes of textual data.\n",
    "- Semantic Representation: Captures the meanings of words and their relationships.\n",
    "- Ease of Use: Can be integrated into various NLP tasks.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Limited Contextuality: Word2Vec does not account for the order of words, which can be critical in some tasks.\n",
    "- Need for Large Datasets: Achieving good results requires a substantial amount of training text.\n",
    "- Inability to Handle Polysemy: A single word may have multiple meanings, and Word2Vec may not always account for this."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c210135cf8a36b40"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97f7aa56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.112351Z",
     "start_time": "2024-08-04T01:18:10.073166Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing = ['original', 'just tokenization', 'stemming', 'lemmatization', 'stemming(snow) + misspellings',\n",
    "                                                        'lemmatization + misspellings', 'stemming(lanc) + misspellings']\n",
    "df_df_prep = pd.DataFrame(columns=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "073e11fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.153908Z",
     "start_time": "2024-08-04T01:18:10.107748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                               original  \\\n0     An inspiration in all aspects: Fashion, fitnes...   \n1     Apka Apna Awam Ka Channel Frankline Tv Aam Adm...   \n2     Beautiful album from  the greatest unsung guit...   \n3     Good luck to Rich riding for great project in ...   \n4               Omg he... kissed... him crying with joy   \n...                                                 ...   \n1019  Supreme Court shoots down govt bid to put spor...   \n1020  Historian Ram Guha, IDFC official Vikram Limay...   \n1021  Supreme Court names former CAG as head of 4-me...   \n1022  Court summons CM suspended BJP MP as accused i...   \n1023  Amulya Patnaik has been appointed new Delhi po...   \n\n                                      just tokenization  \\\n0     an inspiration in all aspects fashion fitness ...   \n1     apka apna awam ka channel frankline tv aam adm...   \n2     beautiful album from the greatest unsung guita...   \n3     good luck to rich riding for great project in ...   \n4                     omg he kissed him crying with joy   \n...                                                 ...   \n1019  supreme court shoots down govt bid to put spor...   \n1020  historian ram guha idfc official vikram limaye...   \n1021  supreme court names former cag as head of memb...   \n1022  court summons cm suspended bjp mp as accused i...   \n1023  amulya patnaik has been appointed new delhi po...   \n\n                                               stemming  \\\n0     an inspir in all aspect fashion fit beauti and...   \n1     apka apna awam ka channel franklin tv aam admi...   \n2     beauti album from the greatest unsung guitar g...   \n3     good luck to rich ride for great project in th...   \n4                          omg he kiss him cri with joy   \n...                                                 ...   \n1019  suprem court shoot down govt bid to put sport ...   \n1020  historian ram guha idfc offici vikram limay fo...   \n1021  suprem court name former cag as head of member...   \n1022  court summon cm suspend bjp mp as accus in cri...   \n1023  amulya patnaik has been appoint new delhi poli...   \n\n                                          lemmatization  \\\n0     an inspiration in all aspect fashion fitness b...   \n1     apka apna awam ka channel frankline tv aam adm...   \n2     beautiful album from the greatest unsung guita...   \n3     good luck to rich riding for great project in ...   \n4                        omg he kissed him cry with joy   \n...                                                 ...   \n1019  supreme court shoot down govt bid to put sport...   \n1020  historian ram guha idfc official vikram limaye...   \n1021  supreme court name former cag a head of member...   \n1022  court summons cm suspended bjp mp a accused in...   \n1023  amulya patnaik ha been appointed new delhi pol...   \n\n                          stemming(snow) + misspellings  \\\n0     an inspir in all aspect fashion fit beauti and...   \n1     aka anna away ka channel franklin to am admit ...   \n2     beauti album from the greatest unsung guitar g...   \n3     good luck to rich ride for great project in th...   \n4                           om he kiss him cri with joy   \n...                                                 ...   \n1019  suprem court shoot down got bid to put sport m...   \n1020  historian ram gula if offici viral imag former...   \n1021  suprem court name former can as head of member...   \n1022  court summon am suspend bop my as accus in cri...   \n1023  amulet has been appoint new deli polic commiss...   \n\n                           lemmatization + misspellings  \\\n0     an inspiration in all aspect fashion fitness b...   \n1     aka anna away ka channel franklin to am admit ...   \n2     beautiful album from the greatest unsung guita...   \n3     good luck to rich riding for great project in ...   \n4                         om he kissed him cry with joy   \n...                                                 ...   \n1019  supreme court shoot down got bid to put sport ...   \n1020  historian ram gula if official viral image for...   \n1021  supreme court name former can a head of member...   \n1022  court summons am suspended bop my a accused in...   \n1023  amulet ha been appointed new deli police commi...   \n\n                          stemming(lanc) + misspellings  \n0     an inspir in al aspect fash fit beauty and per...  \n1     ak ann away ka channel franklin to am admit pr...  \n2     beauty alb from the greatest unsung guit geni ...  \n3     good luck to rich rid for gre project in thi s...  \n4                           om he kiss him cry with joy  \n...                                                 ...  \n1019  suprem court shoot down got bid to put sport m...  \n1020  hist ram gul if off vir im form captain lian a...  \n1021  suprem court nam form can as head of memb pane...  \n1022  court summon am suspend bop my as accus in cri...  \n1023  amulet has been appoint new del pol commit is ...  \n\n[2715 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>just tokenization</th>\n      <th>stemming</th>\n      <th>lemmatization</th>\n      <th>stemming(snow) + misspellings</th>\n      <th>lemmatization + misspellings</th>\n      <th>stemming(lanc) + misspellings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>An inspiration in all aspects: Fashion, fitnes...</td>\n      <td>an inspiration in all aspects fashion fitness ...</td>\n      <td>an inspir in all aspect fashion fit beauti and...</td>\n      <td>an inspiration in all aspect fashion fitness b...</td>\n      <td>an inspir in all aspect fashion fit beauti and...</td>\n      <td>an inspiration in all aspect fashion fitness b...</td>\n      <td>an inspir in al aspect fash fit beauty and per...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Adm...</td>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>apka apna awam ka channel franklin tv aam admi...</td>\n      <td>apka apna awam ka channel frankline tv aam adm...</td>\n      <td>aka anna away ka channel franklin to am admit ...</td>\n      <td>aka anna away ka channel franklin to am admit ...</td>\n      <td>ak ann away ka channel franklin to am admit pr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Beautiful album from  the greatest unsung guit...</td>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>beauti album from the greatest unsung guitar g...</td>\n      <td>beautiful album from the greatest unsung guita...</td>\n      <td>beauty alb from the greatest unsung guit geni ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good luck to Rich riding for great project in ...</td>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>good luck to rich ride for great project in th...</td>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>good luck to rich ride for great project in th...</td>\n      <td>good luck to rich riding for great project in ...</td>\n      <td>good luck to rich rid for gre project in thi s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Omg he... kissed... him crying with joy</td>\n      <td>omg he kissed him crying with joy</td>\n      <td>omg he kiss him cri with joy</td>\n      <td>omg he kissed him cry with joy</td>\n      <td>om he kiss him cri with joy</td>\n      <td>om he kissed him cry with joy</td>\n      <td>om he kiss him cry with joy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1019</th>\n      <td>Supreme Court shoots down govt bid to put spor...</td>\n      <td>supreme court shoots down govt bid to put spor...</td>\n      <td>suprem court shoot down govt bid to put sport ...</td>\n      <td>supreme court shoot down govt bid to put sport...</td>\n      <td>suprem court shoot down got bid to put sport m...</td>\n      <td>supreme court shoot down got bid to put sport ...</td>\n      <td>suprem court shoot down got bid to put sport m...</td>\n    </tr>\n    <tr>\n      <th>1020</th>\n      <td>Historian Ram Guha, IDFC official Vikram Limay...</td>\n      <td>historian ram guha idfc official vikram limaye...</td>\n      <td>historian ram guha idfc offici vikram limay fo...</td>\n      <td>historian ram guha idfc official vikram limaye...</td>\n      <td>historian ram gula if offici viral imag former...</td>\n      <td>historian ram gula if official viral image for...</td>\n      <td>hist ram gul if off vir im form captain lian a...</td>\n    </tr>\n    <tr>\n      <th>1021</th>\n      <td>Supreme Court names former CAG as head of 4-me...</td>\n      <td>supreme court names former cag as head of memb...</td>\n      <td>suprem court name former cag as head of member...</td>\n      <td>supreme court name former cag a head of member...</td>\n      <td>suprem court name former can as head of member...</td>\n      <td>supreme court name former can a head of member...</td>\n      <td>suprem court nam form can as head of memb pane...</td>\n    </tr>\n    <tr>\n      <th>1022</th>\n      <td>Court summons CM suspended BJP MP as accused i...</td>\n      <td>court summons cm suspended bjp mp as accused i...</td>\n      <td>court summon cm suspend bjp mp as accus in cri...</td>\n      <td>court summons cm suspended bjp mp a accused in...</td>\n      <td>court summon am suspend bop my as accus in cri...</td>\n      <td>court summons am suspended bop my a accused in...</td>\n      <td>court summon am suspend bop my as accus in cri...</td>\n    </tr>\n    <tr>\n      <th>1023</th>\n      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n      <td>amulya patnaik has been appointed new delhi po...</td>\n      <td>amulya patnaik has been appoint new delhi poli...</td>\n      <td>amulya patnaik ha been appointed new delhi pol...</td>\n      <td>amulet has been appoint new deli polic commiss...</td>\n      <td>amulet ha been appointed new deli police commi...</td>\n      <td>amulet has been appoint new del pol commit is ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2715 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for el in preprocessing:\n",
    "    if el == 'original':\n",
    "        df_df_prep[el] = df['tweets'].iloc[:]\n",
    "    else:\n",
    "        df_df_prep[el] = df_df.iloc[:, 0][el].iloc[:, 0].index\n",
    "df_df_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d87ad52d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.211221Z",
     "start_time": "2024-08-04T01:18:10.152853Z"
    }
   },
   "outputs": [],
   "source": [
    "df_df_prep.to_csv('res/df_df_prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5dbde5",
   "metadata": {},
   "source": [
    "#### Word2Vec examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b06469cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.453451Z",
     "start_time": "2024-08-04T01:18:10.210472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.09505568,  0.3937941 ,  0.04392887, -0.33067578,  0.10746714,\n       -0.91409445,  0.730753  ,  1.1085308 , -0.33851627, -0.50586295,\n       -0.15899393, -0.93298036, -0.14942716, -0.01470097,  0.13183391,\n       -0.29866737, -0.02443575, -0.3835428 , -0.06543013, -0.93508136,\n        0.3376409 ,  0.15310737,  0.49957466, -0.30281562, -0.1575916 ,\n        0.04224298, -0.1382828 , -0.34044975, -0.24866405, -0.03870746,\n        0.5451863 , -0.14133126,  0.30855674, -0.56088084, -0.1796257 ,\n        0.8269225 ,  0.2414299 , -0.05726096, -0.3344391 , -0.55864376,\n        0.26846048, -0.45957068, -0.23192333,  0.00735891,  0.42723098,\n       -0.34936285, -0.40866336, -0.2465668 ,  0.3900131 ,  0.22368641,\n        0.02124124, -0.4222775 ,  0.2863718 ,  0.03347119, -0.32226196,\n        0.29391056,  0.03812635, -0.12255003, -0.2826103 ,  0.10834589,\n       -0.04517619, -0.12301224,  0.33927235, -0.09603848, -0.41030037,\n        0.683605  ,  0.18159644,  0.5405732 , -0.61853415,  0.3424348 ,\n       -0.2270356 ,  0.60450196,  0.7157715 ,  0.13981603,  0.49143898,\n        0.23380923,  0.09384734, -0.14836556, -0.48198652,  0.23957212,\n       -0.28735867, -0.04481005, -0.35729194,  0.28355294, -0.05381286,\n       -0.25124106,  0.2361425 ,  0.28242296,  0.44528168,  0.16984496,\n        0.6102256 ,  0.36998007,  0.0581083 , -0.01286393,  0.8004722 ,\n        0.38238668,  0.2647312 , -0.4140568 ,  0.06114378,  0.11052569],\n      dtype=float32)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "df_df_ = df_df_prep.iloc[:, -2].astype(str).tolist()\n",
    "tweets_lists = [text_to_word_sequence(tw) for tw in df_df_]\n",
    "w2v_model = Word2Vec(sentences=tweets_lists, vector_size=100,\n",
    "\t\t\t\t\t\t \t\t\t\t\t \t\twindow=5, min_count=1)\n",
    "w2v_model.wv.word_vec('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "138f832e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.503928Z",
     "start_time": "2024-08-04T01:18:10.452802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('pick', 0.9152530431747437),\n ('fly', 0.9139841794967651),\n ('kind', 0.9126733541488647),\n ('bad', 0.9119454026222229),\n ('start', 0.9114742875099182),\n ('gone', 0.9113501906394958),\n ('shot', 0.9105678200721741),\n ('free', 0.9102700352668762),\n ('reading', 0.9102481007575989),\n ('already', 0.9101448655128479)]"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e10128d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:18:10.554656Z",
     "start_time": "2024-08-04T01:18:10.502933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'guitar'"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(\"girl guitar man woman\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac23c5",
   "metadata": {},
   "source": [
    "### Using  pretrained word vector representation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5407873",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> #### glove-twitter-25"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GloVe-Twitter-25** is a pre-trained word vector representation model designed for processing texts from Twitter. It is based on the GloVe (Global Vectors for Word Representation) method, which creates vector representations of words by considering word statistics in a large text corpus. The GloVe-Twitter-25 model was trained on a large dataset of tweets and has a vector size of 25.\n",
    "\n",
    "#### Key Principles\n",
    "1. **GloVe Method**:\n",
    "   - GloVe uses a word co-occurrence matrix to study how often words appear together in text. This allows capturing contextual relationships between words based on their frequency of occurrence in various contexts.\n",
    "   - The model creates vectors such that their scalar products reflect the logarithm of the probability of co-occurrence of words.\n",
    "\n",
    "2. **Training on Twitter**:\n",
    "   - GloVe-Twitter-25 was trained on millions of tweets, allowing it to capture the specific nuances of Twitter language, such as slang, abbreviations, and emojis.\n",
    "   - Training on such a corpus makes the model particularly useful for tasks related to social media analysis and natural language processing in the context of Twitter.\n",
    "\n",
    "3. **Vector Size**:\n",
    "   - The vectors have a dimension of 25, making them compact and convenient for use in various applications where processing speed and memory efficiency are critical.\n",
    "\n",
    "#### Applications\n",
    "- **Sentiment Analysis**: Used to determine the emotional tone of tweets, which is useful for marketing research and public opinion monitoring.\n",
    "- **Text Classification**: Applied for automatic categorization of tweets based on their content.\n",
    "- **Recommendation Systems**: Helps in creating recommendations based on textual content in social networks.\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Specificity: Training on Twitter data allows the model to better handle the language and style characteristic of this platform.\n",
    "- Compactness: The small vector size (25) makes them easy to integrate into applications with limited resources.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Limited Dimensionality: The smaller vector size may limit the model's ability to capture more complex semantic relationships compared to models with larger dimensions.\n",
    "- Language Specificity: The model may not be suitable for tasks requiring processing of texts outside the Twitter context or in more formal language settings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3096b879c81924f2"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94cb717e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:25:55.257929Z",
     "start_time": "2024-08-04T01:18:10.552859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-1.2304 ,  0.48312,  0.14102, -0.0295 , -0.65253, -0.18554,\n        2.1033 ,  1.7516 , -1.3001 , -0.32113, -0.84774,  0.41995,\n       -3.8823 ,  0.19638, -0.72865, -0.85273,  0.23174, -1.0763 ,\n       -0.83023,  0.10815, -0.51015,  0.27691, -1.1895 ,  0.98094,\n       -0.13955], dtype=float32)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_corpus = api.load('glove-twitter-25')\n",
    "w2v_corpus['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86b271cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:25:55.258384Z",
     "start_time": "2024-08-04T01:19:31.104566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('lifestyle', 0.877960205078125),\n ('wellness', 0.8659428954124451),\n ('production', 0.8604754209518433),\n ('skills', 0.8599393963813782),\n ('training', 0.8574686646461487),\n ('professional', 0.853054940700531),\n ('crossfit', 0.8482766151428223),\n ('nutrition', 0.8449391722679138),\n ('yoga', 0.8430234789848328),\n ('coaching', 0.8291710019111633)]"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.most_similar('fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c62c2602",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:25:55.258695Z",
     "start_time": "2024-08-04T01:19:31.204372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'apple'"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.doesnt_match(\"girl apple man woman\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c2588",
   "metadata": {},
   "source": [
    "> ### glove-twitter-100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GloVe-Twitter-100** is a pre-trained word vector representation model designed for processing texts from Twitter. It is based on the GloVe (Global Vectors for Word Representation) method and was trained on a large dataset of tweets. The vector size in this model is 100, allowing it to capture more complex semantic relationships between words compared to lower-dimensional models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122673e3a6604bf6"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b696ff4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:26:03.592052Z",
     "start_time": "2024-08-04T01:19:31.212402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([ 0.023098 , -0.11098  ,  0.079839 ,  0.26566  ,  0.23083  ,\n       -0.14683  ,  0.29009  ,  0.24811  , -0.38742  ,  0.11899  ,\n       -0.81393  , -0.69197  , -4.0274   , -0.096299 , -0.49273  ,\n        0.71179  ,  0.043593 ,  0.048169 , -0.90247  ,  0.23704  ,\n        0.20754  , -0.10822  , -0.69071  , -0.33782  ,  0.83584  ,\n       -0.75044  ,  0.21905  ,  0.28662  ,  0.63882  , -1.0862   ,\n       -0.76783  , -0.4843   ,  0.34029  ,  0.65897  ,  0.50015  ,\n        0.52957  ,  0.39435  , -0.38319  ,  0.11514  , -0.1388   ,\n       -1.3666   ,  0.1397   ,  0.18929  ,  0.93266  , -0.47246  ,\n       -0.19455  , -0.03649  , -0.98943  , -0.27461  ,  0.24763  ,\n       -0.45024  , -0.71812  ,  0.61547  , -0.90039  ,  0.92341  ,\n        0.2597   ,  0.058149 ,  0.30903  ,  0.26106  , -0.087882 ,\n       -0.18843  , -0.85732  ,  0.065188 ,  0.035417 , -0.1342   ,\n        0.06486  , -1.07     , -0.37303  , -0.79469  ,  0.23944  ,\n        0.37891  , -0.36431  ,  0.19694  ,  0.39264  ,  0.65877  ,\n        0.76098  , -0.24817  ,  0.0091144,  0.027382 ,  0.44741  ,\n        1.5235   , -0.013151 ,  0.31054  ,  1.1684   , -0.092836 ,\n        0.30074  , -0.42892  ,  0.51438  ,  0.10516  ,  0.034789 ,\n        0.48588  , -0.49273  ,  0.46741  ,  0.37783  ,  0.55384  ,\n       -0.101    , -0.3925   ,  0.11309  , -0.1613   , -1.0184   ],\n      dtype=float32)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus = api.load('glove-twitter-100')\n",
    "w2v_corpus['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72d1c87b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:26:03.592638Z",
     "start_time": "2024-08-04T01:23:48.803106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('workout', 0.7857903838157654),\n ('crossfit', 0.7615430355072021),\n ('training', 0.7413853406906128),\n ('bodybuilding', 0.7236645817756653),\n ('wellness', 0.7222222089767456),\n ('cardio', 0.7207351922988892),\n ('workouts', 0.7059327363967896),\n ('nutrition', 0.7046872973442078),\n ('gym', 0.7045153975486755),\n ('exercise', 0.7042489051818848)]"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.most_similar('fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "080c3514",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T01:26:03.593075Z",
     "start_time": "2024-08-04T01:23:49.098276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'apple'"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.doesnt_match(\"girl apple man woman\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c1743",
   "metadata": {},
   "source": [
    "> ### glove-twitter-200"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GloVe-Twitter-200** is a pre-trained word vector representation model designed for processing texts from Twitter. It is based on the GloVe (Global Vectors for Word Representation) method and was trained on a large dataset of tweets. The vector size in this model is 200, allowing it to capture more complex semantic relationships between words compared to lower-dimensional models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d555f030c66f8477"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5534a261",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:51:29.720741Z",
     "start_time": "2024-08-04T01:23:49.146196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([ 3.4055e-01, -3.5341e-02,  1.7932e-01,  2.2748e-01,  5.1800e-01,\n        3.5620e-01,  4.7427e-01, -3.3973e-01, -1.1459e-01, -8.5816e-02,\n       -4.8371e-01, -1.5185e-01,  4.9683e-02,  2.3031e-01, -1.2894e-02,\n        4.2952e-01, -2.2993e-01,  4.0219e-01, -4.6905e-01, -3.4270e-01,\n        2.3068e-01,  5.4987e-02, -6.3739e-01, -1.7282e-01,  1.2480e-01,\n        4.7597e-01, -3.1538e-01, -2.3897e-01,  6.3453e-01,  6.9128e-02,\n        4.4254e-02, -1.7599e-01,  2.4331e-01,  8.8688e-01, -2.1671e-02,\n        3.5471e-01,  5.1198e-01,  3.7152e-01, -3.1553e-01,  1.9369e-01,\n       -2.3263e-01, -8.4731e-02,  3.2064e-01,  4.1194e-01, -7.6711e-01,\n       -2.3819e-01, -2.2367e-02, -4.8029e-01, -2.1130e-01,  3.7667e-01,\n       -3.8449e-01, -4.6203e-01,  2.0125e-01, -8.7098e-01,  6.3067e-01,\n        3.8002e-01,  1.0009e-01,  2.2057e-01,  1.2709e-01, -2.7291e-01,\n       -3.8695e-01, -1.7037e-01, -5.2444e-01,  1.6979e-01, -6.3892e-02,\n       -4.1493e-01, -3.0092e-01, -8.8667e-02,  2.7442e-02, -3.3296e-02,\n        8.9140e-02, -5.1032e-01,  2.8867e-01, -7.8973e-02,  5.1809e-01,\n        3.7473e-02, -1.8430e-01,  1.2993e-01, -3.5639e-01, -2.3771e-02,\n        2.5608e-01,  5.7367e-01,  1.8977e-01,  4.2735e-01, -6.2950e-01,\n       -6.1607e-02, -2.3547e-01,  3.0832e-01, -9.6577e-02,  8.0204e-02,\n        6.3908e-02,  1.5368e-01,  1.3495e-01,  5.3460e-01,  5.7565e-01,\n       -7.0313e-01, -5.8487e-01,  3.9416e-01,  3.8035e-02, -6.4658e-01,\n       -5.1457e-01,  2.7159e-01, -1.3090e-01,  1.8903e-01,  3.6076e-01,\n        5.1712e-01, -4.3873e-02,  3.8467e-01, -5.1280e-01,  4.4015e-01,\n        8.4145e-01,  2.9589e-01,  5.8459e-01,  2.0025e-01, -2.6151e-01,\n       -1.4161e-01, -4.1055e-01, -7.1253e-01,  3.7635e-03, -3.2234e-02,\n        8.6813e-01,  9.9012e-02, -1.3202e-01,  3.9301e-01,  7.7522e-01,\n       -2.0866e-01, -5.2355e-01, -8.9420e-01, -5.5599e-01,  2.7570e-01,\n        1.4127e-01, -2.2915e-01,  2.5990e-01,  7.2167e-01, -3.9657e-02,\n        1.7373e-01, -4.5132e-01,  8.0132e-02, -3.7991e-02,  3.1517e-01,\n       -3.0152e-01, -4.2371e-01,  5.9759e-01,  5.5957e-01, -3.4958e-01,\n        2.4964e-01,  1.5139e-01,  1.0330e-01,  2.3283e-01, -5.4755e-03,\n        2.1316e-02,  1.5666e-01, -5.0994e+00,  3.3258e-01, -7.6817e-02,\n        9.1090e-02, -7.7267e-01,  2.8711e-01,  2.8031e-01, -3.7526e-01,\n        2.7968e-01, -6.7341e-01, -3.7488e-01,  1.7857e-01, -3.0030e-01,\n       -5.4835e-02,  7.4565e-01,  3.9090e-01,  3.4904e-01,  3.6017e-01,\n       -5.3500e-01,  1.2365e-01,  3.8861e-01,  3.0605e-02,  2.9655e-01,\n        1.2382e-02, -2.5130e-01, -3.1893e-01, -1.5266e-01,  9.5933e-02,\n        2.6980e-04, -2.4731e-01, -7.6809e-01,  1.7810e-01, -1.7828e-02,\n       -7.4917e-01,  2.2210e-03, -4.9883e-03, -7.9147e-01,  4.1020e-01,\n       -7.5138e-01, -1.4041e-01,  6.9163e-01, -1.5143e-01, -1.7265e-01,\n        9.4816e-02, -6.2903e-02, -3.8502e-01, -7.1417e-02, -3.8061e-01],\n      dtype=float32)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus = api.load('glove-twitter-200')\n",
    "w2v_corpus['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37a77ea7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:51:29.721115Z",
     "start_time": "2024-08-04T01:32:03.092466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('workout', 0.7339531183242798),\n ('crossfit', 0.6933854818344116),\n ('gym', 0.6854565143585205),\n ('training', 0.6848605871200562),\n ('nutrition', 0.673112690448761),\n ('wellness', 0.6691936254501343),\n ('exercise', 0.6544373631477356),\n ('cardio', 0.6528776288032532),\n ('bodybuilding', 0.6506840586662292),\n ('pilates', 0.6403564810752869)]"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.most_similar('fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c507e0bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:51:29.721342Z",
     "start_time": "2024-08-04T01:32:03.641277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'apple'"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corpus.doesnt_match(\"girl apple man woman\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b15b8",
   "metadata": {},
   "source": [
    "#### Word2Vec and pretrained word vector representation models with Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9989a18f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:51:58.525271Z",
     "start_time": "2024-08-04T01:32:03.680423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                    w2v glove-25 glove-100 glove-200\njust tokenization              0.701657  0.85267  0.898711  0.909761\nstemming                       0.690608  0.85267  0.898711  0.909761\nlemmatization                  0.709024  0.85267  0.898711  0.909761\nstemming(snow) + misspellings  0.699816  0.85267  0.898711  0.909761\nlemmatization + misspellings   0.690608  0.85267  0.898711  0.909761\nstemming(lanc) + misspellings  0.697974  0.85267  0.898711  0.909761",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>w2v</th>\n      <th>glove-25</th>\n      <th>glove-100</th>\n      <th>glove-200</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.701657</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.690608</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.709024</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.699816</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.690608</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.697974</td>\n      <td>0.85267</td>\n      <td>0.898711</td>\n      <td>0.909761</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "params_best = {'C': 10.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "clf.set_params(**params_best)\n",
    "res = w2v_ml.model_preprocessing(clf, df, df_df_prep)\n",
    "res.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35e83a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Word2Vec and pretrained word vector representation models  with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "caec31b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:52:18.568078Z",
     "start_time": "2024-08-04T01:50:38.494907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                    w2v  glove-25 glove-100 glove-200\njust tokenization              0.552486  0.732965  0.769797  0.755064\nstemming                       0.574586  0.742173  0.755064  0.767956\nlemmatization                  0.550645  0.740331  0.777164   0.73849\nstemming(snow) + misspellings  0.561694   0.71639  0.769797  0.736648\nlemmatization + misspellings   0.565378  0.720074  0.755064  0.744015\nstemming(lanc) + misspellings  0.561694  0.731123  0.769797  0.742173",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>w2v</th>\n      <th>glove-25</th>\n      <th>glove-100</th>\n      <th>glove-200</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.552486</td>\n      <td>0.732965</td>\n      <td>0.769797</td>\n      <td>0.755064</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.574586</td>\n      <td>0.742173</td>\n      <td>0.755064</td>\n      <td>0.767956</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.550645</td>\n      <td>0.740331</td>\n      <td>0.777164</td>\n      <td>0.73849</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.561694</td>\n      <td>0.71639</td>\n      <td>0.769797</td>\n      <td>0.736648</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.565378</td>\n      <td>0.720074</td>\n      <td>0.755064</td>\n      <td>0.744015</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.561694</td>\n      <td>0.731123</td>\n      <td>0.769797</td>\n      <td>0.742173</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "params_best = {'criterion': 'gini', 'max_depth': 16}\n",
    "clf.set_params(**params_best)\n",
    "res = w2v_ml.model_preprocessing(clf, df, df_df_prep)\n",
    "res.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2c68a",
   "metadata": {},
   "source": [
    "#### Word2Vec and pretrained word vector representation models with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab0503ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-08-04T02:52:32.259501Z",
     "start_time": "2024-08-04T02:09:12.066444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                    w2v  glove-25 glove-100 glove-200\njust tokenization               0.74954  0.850829  0.904236  0.896869\nstemming                       0.760589  0.850829  0.904236  0.896869\nlemmatization                  0.755064  0.850829  0.904236  0.896869\nstemming(snow) + misspellings  0.758748  0.850829  0.904236  0.896869\nlemmatization + misspellings   0.755064  0.850829  0.904236  0.896869\nstemming(lanc) + misspellings  0.762431  0.850829  0.904236  0.896869",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>w2v</th>\n      <th>glove-25</th>\n      <th>glove-100</th>\n      <th>glove-200</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>just tokenization</th>\n      <td>0.74954</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n    <tr>\n      <th>stemming</th>\n      <td>0.760589</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n    <tr>\n      <th>lemmatization</th>\n      <td>0.755064</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n    <tr>\n      <th>stemming(snow) + misspellings</th>\n      <td>0.758748</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n    <tr>\n      <th>lemmatization + misspellings</th>\n      <td>0.755064</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n    <tr>\n      <th>stemming(lanc) + misspellings</th>\n      <td>0.762431</td>\n      <td>0.850829</td>\n      <td>0.904236</td>\n      <td>0.896869</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "params_best = {'criterion': 'gini', 'max_depth': 32, 'n_estimators': 1000}\n",
    "clf.set_params(**params_best)\n",
    "res = w2v_ml.model_preprocessing(clf, df, df_df_prep)\n",
    "res.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02bea4",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d277e",
   "metadata": {},
   "source": [
    "### Some useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9abf4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "- sentiment analysis https://www.analyticsvidhya.com/blog/2021/09/sentiment-classification-using-nlp-with-text-analytics/\n",
    "- https://becominghuman.ai/nlp-classifying-positive-and-negative-restaurant-reviews-bag-of-words-model-31e9abfd7286\n",
    "- Comments classification https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "- tokenization https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4\n",
    "- Lemmatization https://pythobyte.com/stemming-and-lemmatization-82464/\n",
    "- Fundamentals of Bag Of Words and TF-IDF https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22\n",
    "- How to Vectorize Text in DataFrames for NLP Tasks https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "- Stemming: Porter Vs. Snowball Vs. Lancaster https://towardsai.net/p/l/stemming-porter-vs-snowball-vs-lancaster\n",
    "- Stemming Ð¸ Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð² Python https://pythobyte.com/stemming-and-lemmatization-82464/\n",
    "- https://www.bigdataschool.ru/blog/pyspark-vectorization.html\n",
    "- https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n",
    "- https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "- different stemmers https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/\n",
    "- preprocessing https://dataaspirant.com/nlp-text-preprocessing-techniques-implementation-python/#t-1600081660724"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
